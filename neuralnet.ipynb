{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b86089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2edff342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: keras in ./.venv/lib/python3.12/site-packages (3.11.3)\n",
      "Requirement already satisfied: absl-py in ./.venv/lib/python3.12/site-packages (from keras) (2.3.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from keras) (2.3.2)\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.12/site-packages (from keras) (14.1.0)\n",
      "Requirement already satisfied: namex in ./.venv/lib/python3.12/site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: h5py in ./.venv/lib/python3.12/site-packages (from keras) (3.14.0)\n",
      "Requirement already satisfied: optree in ./.venv/lib/python3.12/site-packages (from keras) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in ./.venv/lib/python3.12/site-packages (from keras) (0.5.3)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from keras) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in ./.venv/lib/python3.12/site-packages (from optree->keras) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in ./.venv/lib/python3.12/site-packages (from tensorflow) (4.15.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.74.0-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: keras>=3.10.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.3.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.5.3)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in ./.venv/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in ./.venv/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-macosx_12_0_arm64.whl (200.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:00:19\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.74.0-cp312-cp312-macosx_11_0_universal2.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl (205 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading wrapt-1.17.3-cp312-cp312-macosx_11_0_arm64.whl (39 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, termcolor, tensorboard-data-server, setuptools, protobuf, opt_einsum, MarkupSafe, markdown, idna, grpcio, google_pasta, gast, charset_normalizer, certifi, werkzeug, requests, astunparse, tensorboard, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/23\u001b[0m [tensorflow]3\u001b[0m [tensorflow]]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 astunparse-1.6.3 certifi-2025.8.3 charset_normalizer-3.4.3 flatbuffers-25.2.10 gast-0.6.0 google_pasta-0.2.0 grpcio-1.74.0 idna-3.10 libclang-18.1.1 markdown-3.9 opt_einsum-3.4.0 protobuf-6.32.0 requests-2.32.5 setuptools-80.9.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 urllib3-2.5.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "#Install Packages\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fffb1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be520d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LAYER ===\n",
    "class Layer():\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "      self.weights = np.random.randn(num_inputs, num_outputs) * np.sqrt(2.0 / (num_inputs+num_outputs)) \n",
    "      self.biases = np.random.randn(1, num_outputs) \n",
    "\n",
    "    def forward(self, inputs):\n",
    "      self.inputs = inputs\n",
    "      self.outputs = np.dot(self.inputs, self.weights) + self.biases \n",
    "\n",
    "    def backward(self, del_z):\n",
    "      batch_size = self.inputs.shape[0]\n",
    "\n",
    "      self.del_w = np.dot(self.inputs.T, del_z) / batch_size\n",
    "      self.del_b = np.sum(del_z, axis=0, keepdims=True) / batch_size # Sum over batch dimension\n",
    "      self.del_z_prev = np.dot(del_z, self.weights.T) \n",
    "\n",
    "# === ACTIVATION: RELU ===\n",
    "class ReLU():\n",
    "    def forward(self, inputs):\n",
    "      self.inputs = inputs\n",
    "      self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, del_z):\n",
    "      self.del_z_prev = del_z.copy()\n",
    "      self.del_z_prev[self.inputs <= 0] = 0\n",
    "\n",
    "# === ACTIVATION: SOFTMAX ===\n",
    "class Softmax():\n",
    "    def forward(self, inputs):\n",
    "      exp_z = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "      self.output = exp_z / np.sum(exp_z, axis=1, keepdims=True) \n",
    "\n",
    "# === LOSS: CROSS-ENTROPY ===\n",
    "class CategoricalCrossentropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        else:\n",
    "            correct_confidences = y_pred_clipped[range(len(y_pred_clipped)), y_true]\n",
    "        return -np.mean(np.log(correct_confidences))\n",
    "\n",
    "# === COMBINED SOFTMAX + CROSS-ENTROPY BACKWARD ===\n",
    "class Softmax_CategoricalCrossentropy:\n",
    "    def backward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        self.del_z_prev = y_pred.copy()\n",
    "        self.del_z_prev[range(samples), y_true] -= 1 \n",
    "        self.del_z_prev = self.del_z_prev / samples\n",
    "\n",
    "# === OPTIMIZER: SGD ===\n",
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        layer.weights -= self.lr * layer.del_w\n",
    "        layer.biases  -= self.lr * layer.del_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5755be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MLP ===\n",
    "# Allow for flexible number of hidden layers\n",
    "class MLP:\n",
    "    def __init__(self, layers:list):\n",
    "      self.layer_count = len(layers)\n",
    "      self.hidden_layers = self.layer_count - 1\n",
    "      self.layers = [Layer(layers[i], layers[i+1]) for i in range(self.hidden_layers)]\n",
    "      self.activations = [ReLU() for _ in range(self.hidden_layers - 1)] + [Softmax()]\n",
    "      self.loss = CategoricalCrossentropy()\n",
    "      self.loss_activation = Softmax_CategoricalCrossentropy() \n",
    "      self.optimizer = SGD()\n",
    "\n",
    "\n",
    "    # Forward Propagation\n",
    "    def forward(self, x):\n",
    "      current = x\n",
    "      for layer, activation in zip(self.layers, self.activations):\n",
    "        layer.forward(current)              \n",
    "        activation.forward(layer.outputs)   \n",
    "        current = activation.output         \n",
    "      return current\n",
    "\n",
    "\n",
    "    def backward(self, y_true):\n",
    "        self.loss_activation.backward(self.activations[-1].output, y_true)\n",
    "        grad_from_next_layer = self.loss_activation.del_z_prev \n",
    "        self.layers[-1].backward(grad_from_next_layer)\n",
    "        grad_from_next_layer = self.layers[-1].del_z_prev \n",
    "\n",
    "        self.optimizer.update_params(self.layers[-1])\n",
    "\n",
    "        for i in reversed(range(len(self.layers) - 1)):\n",
    "            current_activation = self.activations[i]  \n",
    "            current_layer = self.layers[i]            \n",
    "            current_activation.backward(grad_from_next_layer)\n",
    "            grad_to_layer_backward = current_activation.del_z_prev \n",
    "            current_layer.backward(grad_to_layer_backward)\n",
    "            grad_from_next_layer = current_layer.del_z_prev \n",
    "\n",
    "    def fit(self, X_train, y_train, X_test, y_test, epochs=10, batch_size=64):\n",
    "        \n",
    "        # Data preparation (normalization)\n",
    "        X_train = X_train / 255.0\n",
    "        X_test = X_test / 255.0\n",
    "        \n",
    "        # One-hot encode the labels\n",
    "        y_train_one_hot = np.zeros((y_train.size, y_train.max() + 1))\n",
    "        y_train_one_hot[np.arange(y_train.size), y_train] = 1\n",
    "        \n",
    "        y_test_one_hot = np.zeros((y_test.size, y_test.max() + 1))\n",
    "        y_test_one_hot[np.arange(y_test.size), y_test] = 1\n",
    "\n",
    "        num_samples = len(X_train)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Mini-batch loop\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                batch_indices = indices[i:i + batch_size]\n",
    "                X_batch = X_train[batch_indices]\n",
    "                y_batch = y_train_one_hot[batch_indices]\n",
    "\n",
    "                y_pred = self.forward(X_batch)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(y_batch)\n",
    "                \n",
    "                # Calculate and accumulate loss\n",
    "                loss = self.loss.forward(y_pred, y_batch)\n",
    "                epoch_loss += loss\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_loss = epoch_loss / (len(X_train) / batch_size)\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred_train = self.forward(X_train)\n",
    "            y_pred_train_labels = np.argmax(y_pred_train, axis=1)\n",
    "            train_accuracy = np.mean(y_pred_train_labels == y_train)   \n",
    "\n",
    "            # Validation accuracy\n",
    "            y_pred_test = self.forward(X_test)\n",
    "            y_pred_test_labels = np.argmax(y_pred_test, axis=1)\n",
    "            val_accuracy = np.mean(y_pred_test_labels == y_test)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Acc: {val_accuracy:.4f}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4b424ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Training data shape: (54000, 784), labels shape: (54000,)\n",
      "Validation data shape: (6000, 784), labels shape: (6000,)\n",
      "Test data shape: (10000, 784), labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Flatten the images and normalize pixel values (moved into fit function)\n",
    "# Images are 28x28, so each input sample has 784 features\n",
    "X_train_full = X_train_full.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)\n",
    "\n",
    "# Split a validation set from the training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, labels shape: {y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}, labels shape: {y_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}, labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18b6ee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 | Loss: 3.5863 | Train Acc: 0.0998 | Val Acc: 0.1038\n",
      "Epoch 2/1000 | Loss: 3.3266 | Train Acc: 0.1013 | Val Acc: 0.1045\n",
      "Epoch 3/1000 | Loss: 3.1331 | Train Acc: 0.1040 | Val Acc: 0.1065\n",
      "Epoch 4/1000 | Loss: 2.9831 | Train Acc: 0.1080 | Val Acc: 0.1100\n",
      "Epoch 5/1000 | Loss: 2.8631 | Train Acc: 0.1128 | Val Acc: 0.1147\n",
      "Epoch 6/1000 | Loss: 2.7644 | Train Acc: 0.1177 | Val Acc: 0.1195\n",
      "Epoch 7/1000 | Loss: 2.6819 | Train Acc: 0.1240 | Val Acc: 0.1257\n",
      "Epoch 8/1000 | Loss: 2.6119 | Train Acc: 0.1322 | Val Acc: 0.1342\n",
      "Epoch 9/1000 | Loss: 2.5520 | Train Acc: 0.1426 | Val Acc: 0.1467\n",
      "Epoch 10/1000 | Loss: 2.5000 | Train Acc: 0.1529 | Val Acc: 0.1558\n",
      "Epoch 11/1000 | Loss: 2.4549 | Train Acc: 0.1629 | Val Acc: 0.1663\n",
      "Epoch 12/1000 | Loss: 2.4154 | Train Acc: 0.1717 | Val Acc: 0.1752\n",
      "Epoch 13/1000 | Loss: 2.3806 | Train Acc: 0.1789 | Val Acc: 0.1832\n",
      "Epoch 14/1000 | Loss: 2.3498 | Train Acc: 0.1854 | Val Acc: 0.1910\n",
      "Epoch 15/1000 | Loss: 2.3226 | Train Acc: 0.1901 | Val Acc: 0.1960\n",
      "Epoch 16/1000 | Loss: 2.2983 | Train Acc: 0.1936 | Val Acc: 0.2003\n",
      "Epoch 17/1000 | Loss: 2.2766 | Train Acc: 0.1956 | Val Acc: 0.2017\n",
      "Epoch 18/1000 | Loss: 2.2572 | Train Acc: 0.1971 | Val Acc: 0.2007\n",
      "Epoch 19/1000 | Loss: 2.2398 | Train Acc: 0.1990 | Val Acc: 0.2010\n",
      "Epoch 20/1000 | Loss: 2.2240 | Train Acc: 0.2003 | Val Acc: 0.2012\n",
      "Epoch 21/1000 | Loss: 2.2098 | Train Acc: 0.2012 | Val Acc: 0.2020\n",
      "Epoch 22/1000 | Loss: 2.1969 | Train Acc: 0.2025 | Val Acc: 0.2040\n",
      "Epoch 23/1000 | Loss: 2.1853 | Train Acc: 0.2034 | Val Acc: 0.2057\n",
      "Epoch 24/1000 | Loss: 2.1746 | Train Acc: 0.2051 | Val Acc: 0.2097\n",
      "Epoch 25/1000 | Loss: 2.1650 | Train Acc: 0.2070 | Val Acc: 0.2095\n",
      "Epoch 26/1000 | Loss: 2.1561 | Train Acc: 0.2108 | Val Acc: 0.2112\n",
      "Epoch 27/1000 | Loss: 2.1480 | Train Acc: 0.2148 | Val Acc: 0.2155\n",
      "Epoch 28/1000 | Loss: 2.1405 | Train Acc: 0.2192 | Val Acc: 0.2213\n",
      "Epoch 29/1000 | Loss: 2.1337 | Train Acc: 0.2237 | Val Acc: 0.2263\n",
      "Epoch 30/1000 | Loss: 2.1273 | Train Acc: 0.2277 | Val Acc: 0.2300\n",
      "Epoch 31/1000 | Loss: 2.1214 | Train Acc: 0.2326 | Val Acc: 0.2337\n",
      "Epoch 32/1000 | Loss: 2.1159 | Train Acc: 0.2372 | Val Acc: 0.2377\n",
      "Epoch 33/1000 | Loss: 2.1107 | Train Acc: 0.2431 | Val Acc: 0.2455\n",
      "Epoch 34/1000 | Loss: 2.1059 | Train Acc: 0.2485 | Val Acc: 0.2518\n",
      "Epoch 35/1000 | Loss: 2.1013 | Train Acc: 0.2537 | Val Acc: 0.2585\n",
      "Epoch 36/1000 | Loss: 2.0970 | Train Acc: 0.2589 | Val Acc: 0.2633\n",
      "Epoch 37/1000 | Loss: 2.0928 | Train Acc: 0.2640 | Val Acc: 0.2690\n",
      "Epoch 38/1000 | Loss: 2.0889 | Train Acc: 0.2695 | Val Acc: 0.2723\n",
      "Epoch 39/1000 | Loss: 2.0851 | Train Acc: 0.2734 | Val Acc: 0.2777\n",
      "Epoch 40/1000 | Loss: 2.0814 | Train Acc: 0.2771 | Val Acc: 0.2810\n",
      "Epoch 41/1000 | Loss: 2.0778 | Train Acc: 0.2808 | Val Acc: 0.2845\n",
      "Epoch 42/1000 | Loss: 2.0744 | Train Acc: 0.2843 | Val Acc: 0.2882\n",
      "Epoch 43/1000 | Loss: 2.0710 | Train Acc: 0.2871 | Val Acc: 0.2908\n",
      "Epoch 44/1000 | Loss: 2.0678 | Train Acc: 0.2902 | Val Acc: 0.2950\n",
      "Epoch 45/1000 | Loss: 2.0645 | Train Acc: 0.2928 | Val Acc: 0.2977\n",
      "Epoch 46/1000 | Loss: 2.0614 | Train Acc: 0.2950 | Val Acc: 0.2975\n",
      "Epoch 47/1000 | Loss: 2.0583 | Train Acc: 0.2970 | Val Acc: 0.3002\n",
      "Epoch 48/1000 | Loss: 2.0552 | Train Acc: 0.2995 | Val Acc: 0.3038\n",
      "Epoch 49/1000 | Loss: 2.0522 | Train Acc: 0.3014 | Val Acc: 0.3075\n",
      "Epoch 50/1000 | Loss: 2.0492 | Train Acc: 0.3037 | Val Acc: 0.3090\n",
      "Epoch 51/1000 | Loss: 2.0463 | Train Acc: 0.3057 | Val Acc: 0.3107\n",
      "Epoch 52/1000 | Loss: 2.0434 | Train Acc: 0.3078 | Val Acc: 0.3117\n",
      "Epoch 53/1000 | Loss: 2.0405 | Train Acc: 0.3097 | Val Acc: 0.3138\n",
      "Epoch 54/1000 | Loss: 2.0376 | Train Acc: 0.3121 | Val Acc: 0.3158\n",
      "Epoch 55/1000 | Loss: 2.0348 | Train Acc: 0.3139 | Val Acc: 0.3183\n",
      "Epoch 56/1000 | Loss: 2.0320 | Train Acc: 0.3157 | Val Acc: 0.3207\n",
      "Epoch 57/1000 | Loss: 2.0292 | Train Acc: 0.3180 | Val Acc: 0.3218\n",
      "Epoch 58/1000 | Loss: 2.0264 | Train Acc: 0.3203 | Val Acc: 0.3240\n",
      "Epoch 59/1000 | Loss: 2.0236 | Train Acc: 0.3227 | Val Acc: 0.3248\n",
      "Epoch 60/1000 | Loss: 2.0209 | Train Acc: 0.3250 | Val Acc: 0.3270\n",
      "Epoch 61/1000 | Loss: 2.0181 | Train Acc: 0.3264 | Val Acc: 0.3292\n",
      "Epoch 62/1000 | Loss: 2.0154 | Train Acc: 0.3281 | Val Acc: 0.3303\n",
      "Epoch 63/1000 | Loss: 2.0127 | Train Acc: 0.3300 | Val Acc: 0.3325\n",
      "Epoch 64/1000 | Loss: 2.0100 | Train Acc: 0.3321 | Val Acc: 0.3342\n",
      "Epoch 65/1000 | Loss: 2.0074 | Train Acc: 0.3339 | Val Acc: 0.3360\n",
      "Epoch 66/1000 | Loss: 2.0047 | Train Acc: 0.3359 | Val Acc: 0.3370\n",
      "Epoch 67/1000 | Loss: 2.0021 | Train Acc: 0.3378 | Val Acc: 0.3398\n",
      "Epoch 68/1000 | Loss: 1.9994 | Train Acc: 0.3395 | Val Acc: 0.3427\n",
      "Epoch 69/1000 | Loss: 1.9968 | Train Acc: 0.3413 | Val Acc: 0.3445\n",
      "Epoch 70/1000 | Loss: 1.9942 | Train Acc: 0.3429 | Val Acc: 0.3462\n",
      "Epoch 71/1000 | Loss: 1.9916 | Train Acc: 0.3448 | Val Acc: 0.3477\n",
      "Epoch 72/1000 | Loss: 1.9891 | Train Acc: 0.3466 | Val Acc: 0.3490\n",
      "Epoch 73/1000 | Loss: 1.9865 | Train Acc: 0.3484 | Val Acc: 0.3510\n",
      "Epoch 74/1000 | Loss: 1.9839 | Train Acc: 0.3504 | Val Acc: 0.3532\n",
      "Epoch 75/1000 | Loss: 1.9814 | Train Acc: 0.3520 | Val Acc: 0.3545\n",
      "Epoch 76/1000 | Loss: 1.9789 | Train Acc: 0.3538 | Val Acc: 0.3555\n",
      "Epoch 77/1000 | Loss: 1.9763 | Train Acc: 0.3556 | Val Acc: 0.3572\n",
      "Epoch 78/1000 | Loss: 1.9738 | Train Acc: 0.3574 | Val Acc: 0.3582\n",
      "Epoch 79/1000 | Loss: 1.9714 | Train Acc: 0.3596 | Val Acc: 0.3603\n",
      "Epoch 80/1000 | Loss: 1.9689 | Train Acc: 0.3616 | Val Acc: 0.3622\n",
      "Epoch 81/1000 | Loss: 1.9664 | Train Acc: 0.3632 | Val Acc: 0.3648\n",
      "Epoch 82/1000 | Loss: 1.9639 | Train Acc: 0.3647 | Val Acc: 0.3657\n",
      "Epoch 83/1000 | Loss: 1.9615 | Train Acc: 0.3669 | Val Acc: 0.3680\n",
      "Epoch 84/1000 | Loss: 1.9591 | Train Acc: 0.3687 | Val Acc: 0.3687\n",
      "Epoch 85/1000 | Loss: 1.9566 | Train Acc: 0.3705 | Val Acc: 0.3703\n",
      "Epoch 86/1000 | Loss: 1.9542 | Train Acc: 0.3722 | Val Acc: 0.3713\n",
      "Epoch 87/1000 | Loss: 1.9518 | Train Acc: 0.3741 | Val Acc: 0.3733\n",
      "Epoch 88/1000 | Loss: 1.9494 | Train Acc: 0.3760 | Val Acc: 0.3750\n",
      "Epoch 89/1000 | Loss: 1.9470 | Train Acc: 0.3777 | Val Acc: 0.3767\n",
      "Epoch 90/1000 | Loss: 1.9446 | Train Acc: 0.3792 | Val Acc: 0.3780\n",
      "Epoch 91/1000 | Loss: 1.9423 | Train Acc: 0.3810 | Val Acc: 0.3795\n",
      "Epoch 92/1000 | Loss: 1.9399 | Train Acc: 0.3825 | Val Acc: 0.3818\n",
      "Epoch 93/1000 | Loss: 1.9376 | Train Acc: 0.3842 | Val Acc: 0.3840\n",
      "Epoch 94/1000 | Loss: 1.9353 | Train Acc: 0.3861 | Val Acc: 0.3863\n",
      "Epoch 95/1000 | Loss: 1.9329 | Train Acc: 0.3877 | Val Acc: 0.3875\n",
      "Epoch 96/1000 | Loss: 1.9306 | Train Acc: 0.3893 | Val Acc: 0.3897\n",
      "Epoch 97/1000 | Loss: 1.9283 | Train Acc: 0.3909 | Val Acc: 0.3915\n",
      "Epoch 98/1000 | Loss: 1.9261 | Train Acc: 0.3927 | Val Acc: 0.3935\n",
      "Epoch 99/1000 | Loss: 1.9238 | Train Acc: 0.3947 | Val Acc: 0.3940\n",
      "Epoch 100/1000 | Loss: 1.9215 | Train Acc: 0.3963 | Val Acc: 0.3967\n",
      "Epoch 101/1000 | Loss: 1.9192 | Train Acc: 0.3978 | Val Acc: 0.3978\n",
      "Epoch 102/1000 | Loss: 1.9170 | Train Acc: 0.3996 | Val Acc: 0.3982\n",
      "Epoch 103/1000 | Loss: 1.9147 | Train Acc: 0.4010 | Val Acc: 0.4000\n",
      "Epoch 104/1000 | Loss: 1.9125 | Train Acc: 0.4023 | Val Acc: 0.4030\n",
      "Epoch 105/1000 | Loss: 1.9103 | Train Acc: 0.4037 | Val Acc: 0.4037\n",
      "Epoch 106/1000 | Loss: 1.9081 | Train Acc: 0.4054 | Val Acc: 0.4040\n",
      "Epoch 107/1000 | Loss: 1.9059 | Train Acc: 0.4070 | Val Acc: 0.4048\n",
      "Epoch 108/1000 | Loss: 1.9037 | Train Acc: 0.4084 | Val Acc: 0.4060\n",
      "Epoch 109/1000 | Loss: 1.9015 | Train Acc: 0.4099 | Val Acc: 0.4078\n",
      "Epoch 110/1000 | Loss: 1.8994 | Train Acc: 0.4109 | Val Acc: 0.4095\n",
      "Epoch 111/1000 | Loss: 1.8972 | Train Acc: 0.4124 | Val Acc: 0.4108\n",
      "Epoch 112/1000 | Loss: 1.8950 | Train Acc: 0.4140 | Val Acc: 0.4118\n",
      "Epoch 113/1000 | Loss: 1.8929 | Train Acc: 0.4150 | Val Acc: 0.4132\n",
      "Epoch 114/1000 | Loss: 1.8908 | Train Acc: 0.4166 | Val Acc: 0.4147\n",
      "Epoch 115/1000 | Loss: 1.8886 | Train Acc: 0.4182 | Val Acc: 0.4155\n",
      "Epoch 116/1000 | Loss: 1.8865 | Train Acc: 0.4199 | Val Acc: 0.4172\n",
      "Epoch 117/1000 | Loss: 1.8844 | Train Acc: 0.4213 | Val Acc: 0.4188\n",
      "Epoch 118/1000 | Loss: 1.8823 | Train Acc: 0.4226 | Val Acc: 0.4205\n",
      "Epoch 119/1000 | Loss: 1.8802 | Train Acc: 0.4242 | Val Acc: 0.4213\n",
      "Epoch 120/1000 | Loss: 1.8781 | Train Acc: 0.4255 | Val Acc: 0.4228\n",
      "Epoch 121/1000 | Loss: 1.8760 | Train Acc: 0.4268 | Val Acc: 0.4237\n",
      "Epoch 122/1000 | Loss: 1.8740 | Train Acc: 0.4281 | Val Acc: 0.4253\n",
      "Epoch 123/1000 | Loss: 1.8719 | Train Acc: 0.4297 | Val Acc: 0.4272\n",
      "Epoch 124/1000 | Loss: 1.8699 | Train Acc: 0.4309 | Val Acc: 0.4292\n",
      "Epoch 125/1000 | Loss: 1.8678 | Train Acc: 0.4322 | Val Acc: 0.4307\n",
      "Epoch 126/1000 | Loss: 1.8658 | Train Acc: 0.4334 | Val Acc: 0.4317\n",
      "Epoch 127/1000 | Loss: 1.8638 | Train Acc: 0.4348 | Val Acc: 0.4333\n",
      "Epoch 128/1000 | Loss: 1.8618 | Train Acc: 0.4362 | Val Acc: 0.4352\n",
      "Epoch 129/1000 | Loss: 1.8597 | Train Acc: 0.4372 | Val Acc: 0.4377\n",
      "Epoch 130/1000 | Loss: 1.8578 | Train Acc: 0.4383 | Val Acc: 0.4393\n",
      "Epoch 131/1000 | Loss: 1.8558 | Train Acc: 0.4396 | Val Acc: 0.4397\n",
      "Epoch 132/1000 | Loss: 1.8538 | Train Acc: 0.4408 | Val Acc: 0.4408\n",
      "Epoch 133/1000 | Loss: 1.8518 | Train Acc: 0.4421 | Val Acc: 0.4428\n",
      "Epoch 134/1000 | Loss: 1.8498 | Train Acc: 0.4435 | Val Acc: 0.4435\n",
      "Epoch 135/1000 | Loss: 1.8479 | Train Acc: 0.4449 | Val Acc: 0.4455\n",
      "Epoch 136/1000 | Loss: 1.8459 | Train Acc: 0.4456 | Val Acc: 0.4472\n",
      "Epoch 137/1000 | Loss: 1.8440 | Train Acc: 0.4469 | Val Acc: 0.4478\n",
      "Epoch 138/1000 | Loss: 1.8420 | Train Acc: 0.4484 | Val Acc: 0.4497\n",
      "Epoch 139/1000 | Loss: 1.8401 | Train Acc: 0.4495 | Val Acc: 0.4515\n",
      "Epoch 140/1000 | Loss: 1.8382 | Train Acc: 0.4507 | Val Acc: 0.4533\n",
      "Epoch 141/1000 | Loss: 1.8363 | Train Acc: 0.4518 | Val Acc: 0.4548\n",
      "Epoch 142/1000 | Loss: 1.8344 | Train Acc: 0.4528 | Val Acc: 0.4550\n",
      "Epoch 143/1000 | Loss: 1.8325 | Train Acc: 0.4541 | Val Acc: 0.4560\n",
      "Epoch 144/1000 | Loss: 1.8306 | Train Acc: 0.4549 | Val Acc: 0.4568\n",
      "Epoch 145/1000 | Loss: 1.8287 | Train Acc: 0.4562 | Val Acc: 0.4583\n",
      "Epoch 146/1000 | Loss: 1.8268 | Train Acc: 0.4570 | Val Acc: 0.4598\n",
      "Epoch 147/1000 | Loss: 1.8250 | Train Acc: 0.4579 | Val Acc: 0.4603\n",
      "Epoch 148/1000 | Loss: 1.8231 | Train Acc: 0.4591 | Val Acc: 0.4622\n",
      "Epoch 149/1000 | Loss: 1.8212 | Train Acc: 0.4601 | Val Acc: 0.4635\n",
      "Epoch 150/1000 | Loss: 1.8194 | Train Acc: 0.4609 | Val Acc: 0.4652\n",
      "Epoch 151/1000 | Loss: 1.8176 | Train Acc: 0.4619 | Val Acc: 0.4663\n",
      "Epoch 152/1000 | Loss: 1.8157 | Train Acc: 0.4629 | Val Acc: 0.4665\n",
      "Epoch 153/1000 | Loss: 1.8139 | Train Acc: 0.4639 | Val Acc: 0.4675\n",
      "Epoch 154/1000 | Loss: 1.8121 | Train Acc: 0.4648 | Val Acc: 0.4688\n",
      "Epoch 155/1000 | Loss: 1.8103 | Train Acc: 0.4657 | Val Acc: 0.4695\n",
      "Epoch 156/1000 | Loss: 1.8085 | Train Acc: 0.4668 | Val Acc: 0.4705\n",
      "Epoch 157/1000 | Loss: 1.8067 | Train Acc: 0.4679 | Val Acc: 0.4715\n",
      "Epoch 158/1000 | Loss: 1.8049 | Train Acc: 0.4687 | Val Acc: 0.4723\n",
      "Epoch 159/1000 | Loss: 1.8031 | Train Acc: 0.4696 | Val Acc: 0.4732\n",
      "Epoch 160/1000 | Loss: 1.8013 | Train Acc: 0.4705 | Val Acc: 0.4740\n",
      "Epoch 161/1000 | Loss: 1.7996 | Train Acc: 0.4715 | Val Acc: 0.4745\n",
      "Epoch 162/1000 | Loss: 1.7978 | Train Acc: 0.4723 | Val Acc: 0.4753\n",
      "Epoch 163/1000 | Loss: 1.7960 | Train Acc: 0.4734 | Val Acc: 0.4773\n",
      "Epoch 164/1000 | Loss: 1.7943 | Train Acc: 0.4744 | Val Acc: 0.4783\n",
      "Epoch 165/1000 | Loss: 1.7925 | Train Acc: 0.4754 | Val Acc: 0.4788\n",
      "Epoch 166/1000 | Loss: 1.7908 | Train Acc: 0.4762 | Val Acc: 0.4798\n",
      "Epoch 167/1000 | Loss: 1.7891 | Train Acc: 0.4769 | Val Acc: 0.4810\n",
      "Epoch 168/1000 | Loss: 1.7873 | Train Acc: 0.4776 | Val Acc: 0.4815\n",
      "Epoch 169/1000 | Loss: 1.7856 | Train Acc: 0.4786 | Val Acc: 0.4822\n",
      "Epoch 170/1000 | Loss: 1.7839 | Train Acc: 0.4793 | Val Acc: 0.4828\n",
      "Epoch 171/1000 | Loss: 1.7822 | Train Acc: 0.4801 | Val Acc: 0.4840\n",
      "Epoch 172/1000 | Loss: 1.7805 | Train Acc: 0.4810 | Val Acc: 0.4845\n",
      "Epoch 173/1000 | Loss: 1.7788 | Train Acc: 0.4818 | Val Acc: 0.4860\n",
      "Epoch 174/1000 | Loss: 1.7771 | Train Acc: 0.4825 | Val Acc: 0.4860\n",
      "Epoch 175/1000 | Loss: 1.7754 | Train Acc: 0.4832 | Val Acc: 0.4867\n",
      "Epoch 176/1000 | Loss: 1.7738 | Train Acc: 0.4836 | Val Acc: 0.4878\n",
      "Epoch 177/1000 | Loss: 1.7721 | Train Acc: 0.4843 | Val Acc: 0.4885\n",
      "Epoch 178/1000 | Loss: 1.7704 | Train Acc: 0.4851 | Val Acc: 0.4890\n",
      "Epoch 179/1000 | Loss: 1.7688 | Train Acc: 0.4859 | Val Acc: 0.4902\n",
      "Epoch 180/1000 | Loss: 1.7671 | Train Acc: 0.4866 | Val Acc: 0.4903\n",
      "Epoch 181/1000 | Loss: 1.7655 | Train Acc: 0.4873 | Val Acc: 0.4907\n",
      "Epoch 182/1000 | Loss: 1.7639 | Train Acc: 0.4880 | Val Acc: 0.4915\n",
      "Epoch 183/1000 | Loss: 1.7622 | Train Acc: 0.4888 | Val Acc: 0.4922\n",
      "Epoch 184/1000 | Loss: 1.7606 | Train Acc: 0.4896 | Val Acc: 0.4928\n",
      "Epoch 185/1000 | Loss: 1.7590 | Train Acc: 0.4904 | Val Acc: 0.4927\n",
      "Epoch 186/1000 | Loss: 1.7574 | Train Acc: 0.4909 | Val Acc: 0.4938\n",
      "Epoch 187/1000 | Loss: 1.7558 | Train Acc: 0.4916 | Val Acc: 0.4942\n",
      "Epoch 188/1000 | Loss: 1.7541 | Train Acc: 0.4924 | Val Acc: 0.4943\n",
      "Epoch 189/1000 | Loss: 1.7525 | Train Acc: 0.4929 | Val Acc: 0.4948\n",
      "Epoch 190/1000 | Loss: 1.7510 | Train Acc: 0.4936 | Val Acc: 0.4953\n",
      "Epoch 191/1000 | Loss: 1.7494 | Train Acc: 0.4944 | Val Acc: 0.4963\n",
      "Epoch 192/1000 | Loss: 1.7478 | Train Acc: 0.4955 | Val Acc: 0.4963\n",
      "Epoch 193/1000 | Loss: 1.7462 | Train Acc: 0.4962 | Val Acc: 0.4972\n",
      "Epoch 194/1000 | Loss: 1.7447 | Train Acc: 0.4967 | Val Acc: 0.4980\n",
      "Epoch 195/1000 | Loss: 1.7431 | Train Acc: 0.4974 | Val Acc: 0.4987\n",
      "Epoch 196/1000 | Loss: 1.7415 | Train Acc: 0.4978 | Val Acc: 0.4995\n",
      "Epoch 197/1000 | Loss: 1.7400 | Train Acc: 0.4986 | Val Acc: 0.5002\n",
      "Epoch 198/1000 | Loss: 1.7384 | Train Acc: 0.4992 | Val Acc: 0.5003\n",
      "Epoch 199/1000 | Loss: 1.7369 | Train Acc: 0.4999 | Val Acc: 0.5015\n",
      "Epoch 200/1000 | Loss: 1.7354 | Train Acc: 0.5006 | Val Acc: 0.5032\n",
      "Epoch 201/1000 | Loss: 1.7338 | Train Acc: 0.5014 | Val Acc: 0.5033\n",
      "Epoch 202/1000 | Loss: 1.7323 | Train Acc: 0.5022 | Val Acc: 0.5038\n",
      "Epoch 203/1000 | Loss: 1.7308 | Train Acc: 0.5029 | Val Acc: 0.5048\n",
      "Epoch 204/1000 | Loss: 1.7292 | Train Acc: 0.5034 | Val Acc: 0.5062\n",
      "Epoch 205/1000 | Loss: 1.7278 | Train Acc: 0.5040 | Val Acc: 0.5067\n",
      "Epoch 206/1000 | Loss: 1.7263 | Train Acc: 0.5049 | Val Acc: 0.5078\n",
      "Epoch 207/1000 | Loss: 1.7247 | Train Acc: 0.5053 | Val Acc: 0.5083\n",
      "Epoch 208/1000 | Loss: 1.7232 | Train Acc: 0.5058 | Val Acc: 0.5092\n",
      "Epoch 209/1000 | Loss: 1.7218 | Train Acc: 0.5065 | Val Acc: 0.5095\n",
      "Epoch 210/1000 | Loss: 1.7203 | Train Acc: 0.5073 | Val Acc: 0.5097\n",
      "Epoch 211/1000 | Loss: 1.7188 | Train Acc: 0.5079 | Val Acc: 0.5103\n",
      "Epoch 212/1000 | Loss: 1.7173 | Train Acc: 0.5084 | Val Acc: 0.5108\n",
      "Epoch 213/1000 | Loss: 1.7158 | Train Acc: 0.5089 | Val Acc: 0.5115\n",
      "Epoch 214/1000 | Loss: 1.7144 | Train Acc: 0.5093 | Val Acc: 0.5127\n",
      "Epoch 215/1000 | Loss: 1.7129 | Train Acc: 0.5099 | Val Acc: 0.5138\n",
      "Epoch 216/1000 | Loss: 1.7115 | Train Acc: 0.5108 | Val Acc: 0.5143\n",
      "Epoch 217/1000 | Loss: 1.7100 | Train Acc: 0.5114 | Val Acc: 0.5150\n",
      "Epoch 218/1000 | Loss: 1.7086 | Train Acc: 0.5119 | Val Acc: 0.5153\n",
      "Epoch 219/1000 | Loss: 1.7071 | Train Acc: 0.5126 | Val Acc: 0.5157\n",
      "Epoch 220/1000 | Loss: 1.7057 | Train Acc: 0.5131 | Val Acc: 0.5162\n",
      "Epoch 221/1000 | Loss: 1.7043 | Train Acc: 0.5135 | Val Acc: 0.5168\n",
      "Epoch 222/1000 | Loss: 1.7028 | Train Acc: 0.5138 | Val Acc: 0.5173\n",
      "Epoch 223/1000 | Loss: 1.7014 | Train Acc: 0.5143 | Val Acc: 0.5190\n",
      "Epoch 224/1000 | Loss: 1.7000 | Train Acc: 0.5150 | Val Acc: 0.5198\n",
      "Epoch 225/1000 | Loss: 1.6986 | Train Acc: 0.5156 | Val Acc: 0.5202\n",
      "Epoch 226/1000 | Loss: 1.6972 | Train Acc: 0.5163 | Val Acc: 0.5208\n",
      "Epoch 227/1000 | Loss: 1.6958 | Train Acc: 0.5166 | Val Acc: 0.5220\n",
      "Epoch 228/1000 | Loss: 1.6944 | Train Acc: 0.5170 | Val Acc: 0.5222\n",
      "Epoch 229/1000 | Loss: 1.6930 | Train Acc: 0.5177 | Val Acc: 0.5233\n",
      "Epoch 230/1000 | Loss: 1.6916 | Train Acc: 0.5183 | Val Acc: 0.5242\n",
      "Epoch 231/1000 | Loss: 1.6902 | Train Acc: 0.5189 | Val Acc: 0.5250\n",
      "Epoch 232/1000 | Loss: 1.6888 | Train Acc: 0.5196 | Val Acc: 0.5257\n",
      "Epoch 233/1000 | Loss: 1.6875 | Train Acc: 0.5199 | Val Acc: 0.5262\n",
      "Epoch 234/1000 | Loss: 1.6861 | Train Acc: 0.5204 | Val Acc: 0.5265\n",
      "Epoch 235/1000 | Loss: 1.6847 | Train Acc: 0.5211 | Val Acc: 0.5270\n",
      "Epoch 236/1000 | Loss: 1.6834 | Train Acc: 0.5216 | Val Acc: 0.5282\n",
      "Epoch 237/1000 | Loss: 1.6820 | Train Acc: 0.5221 | Val Acc: 0.5285\n",
      "Epoch 238/1000 | Loss: 1.6807 | Train Acc: 0.5227 | Val Acc: 0.5290\n",
      "Epoch 239/1000 | Loss: 1.6793 | Train Acc: 0.5233 | Val Acc: 0.5295\n",
      "Epoch 240/1000 | Loss: 1.6779 | Train Acc: 0.5238 | Val Acc: 0.5295\n",
      "Epoch 241/1000 | Loss: 1.6766 | Train Acc: 0.5244 | Val Acc: 0.5302\n",
      "Epoch 242/1000 | Loss: 1.6753 | Train Acc: 0.5248 | Val Acc: 0.5300\n",
      "Epoch 243/1000 | Loss: 1.6740 | Train Acc: 0.5253 | Val Acc: 0.5305\n",
      "Epoch 244/1000 | Loss: 1.6726 | Train Acc: 0.5259 | Val Acc: 0.5308\n",
      "Epoch 245/1000 | Loss: 1.6713 | Train Acc: 0.5264 | Val Acc: 0.5315\n",
      "Epoch 246/1000 | Loss: 1.6700 | Train Acc: 0.5268 | Val Acc: 0.5322\n",
      "Epoch 247/1000 | Loss: 1.6687 | Train Acc: 0.5274 | Val Acc: 0.5323\n",
      "Epoch 248/1000 | Loss: 1.6674 | Train Acc: 0.5280 | Val Acc: 0.5327\n",
      "Epoch 249/1000 | Loss: 1.6661 | Train Acc: 0.5287 | Val Acc: 0.5338\n",
      "Epoch 250/1000 | Loss: 1.6647 | Train Acc: 0.5293 | Val Acc: 0.5343\n",
      "Epoch 251/1000 | Loss: 1.6635 | Train Acc: 0.5299 | Val Acc: 0.5345\n",
      "Epoch 252/1000 | Loss: 1.6622 | Train Acc: 0.5303 | Val Acc: 0.5350\n",
      "Epoch 253/1000 | Loss: 1.6609 | Train Acc: 0.5308 | Val Acc: 0.5358\n",
      "Epoch 254/1000 | Loss: 1.6596 | Train Acc: 0.5311 | Val Acc: 0.5367\n",
      "Epoch 255/1000 | Loss: 1.6583 | Train Acc: 0.5315 | Val Acc: 0.5372\n",
      "Epoch 256/1000 | Loss: 1.6570 | Train Acc: 0.5321 | Val Acc: 0.5375\n",
      "Epoch 257/1000 | Loss: 1.6557 | Train Acc: 0.5326 | Val Acc: 0.5378\n",
      "Epoch 258/1000 | Loss: 1.6545 | Train Acc: 0.5332 | Val Acc: 0.5380\n",
      "Epoch 259/1000 | Loss: 1.6532 | Train Acc: 0.5335 | Val Acc: 0.5388\n",
      "Epoch 260/1000 | Loss: 1.6520 | Train Acc: 0.5339 | Val Acc: 0.5388\n",
      "Epoch 261/1000 | Loss: 1.6507 | Train Acc: 0.5342 | Val Acc: 0.5387\n",
      "Epoch 262/1000 | Loss: 1.6495 | Train Acc: 0.5347 | Val Acc: 0.5393\n",
      "Epoch 263/1000 | Loss: 1.6482 | Train Acc: 0.5352 | Val Acc: 0.5397\n",
      "Epoch 264/1000 | Loss: 1.6469 | Train Acc: 0.5358 | Val Acc: 0.5400\n",
      "Epoch 265/1000 | Loss: 1.6457 | Train Acc: 0.5364 | Val Acc: 0.5402\n",
      "Epoch 266/1000 | Loss: 1.6445 | Train Acc: 0.5370 | Val Acc: 0.5405\n",
      "Epoch 267/1000 | Loss: 1.6432 | Train Acc: 0.5374 | Val Acc: 0.5407\n",
      "Epoch 268/1000 | Loss: 1.6420 | Train Acc: 0.5377 | Val Acc: 0.5408\n",
      "Epoch 269/1000 | Loss: 1.6408 | Train Acc: 0.5381 | Val Acc: 0.5412\n",
      "Epoch 270/1000 | Loss: 1.6396 | Train Acc: 0.5386 | Val Acc: 0.5417\n",
      "Epoch 271/1000 | Loss: 1.6383 | Train Acc: 0.5393 | Val Acc: 0.5420\n",
      "Epoch 272/1000 | Loss: 1.6371 | Train Acc: 0.5396 | Val Acc: 0.5428\n",
      "Epoch 273/1000 | Loss: 1.6359 | Train Acc: 0.5400 | Val Acc: 0.5427\n",
      "Epoch 274/1000 | Loss: 1.6347 | Train Acc: 0.5406 | Val Acc: 0.5428\n",
      "Epoch 275/1000 | Loss: 1.6335 | Train Acc: 0.5411 | Val Acc: 0.5437\n",
      "Epoch 276/1000 | Loss: 1.6323 | Train Acc: 0.5415 | Val Acc: 0.5440\n",
      "Epoch 277/1000 | Loss: 1.6311 | Train Acc: 0.5419 | Val Acc: 0.5443\n",
      "Epoch 278/1000 | Loss: 1.6299 | Train Acc: 0.5423 | Val Acc: 0.5453\n",
      "Epoch 279/1000 | Loss: 1.6287 | Train Acc: 0.5427 | Val Acc: 0.5457\n",
      "Epoch 280/1000 | Loss: 1.6275 | Train Acc: 0.5431 | Val Acc: 0.5460\n",
      "Epoch 281/1000 | Loss: 1.6264 | Train Acc: 0.5434 | Val Acc: 0.5462\n",
      "Epoch 282/1000 | Loss: 1.6252 | Train Acc: 0.5440 | Val Acc: 0.5468\n",
      "Epoch 283/1000 | Loss: 1.6240 | Train Acc: 0.5444 | Val Acc: 0.5470\n",
      "Epoch 284/1000 | Loss: 1.6228 | Train Acc: 0.5448 | Val Acc: 0.5473\n",
      "Epoch 285/1000 | Loss: 1.6216 | Train Acc: 0.5453 | Val Acc: 0.5482\n",
      "Epoch 286/1000 | Loss: 1.6205 | Train Acc: 0.5457 | Val Acc: 0.5485\n",
      "Epoch 287/1000 | Loss: 1.6193 | Train Acc: 0.5461 | Val Acc: 0.5490\n",
      "Epoch 288/1000 | Loss: 1.6182 | Train Acc: 0.5466 | Val Acc: 0.5492\n",
      "Epoch 289/1000 | Loss: 1.6170 | Train Acc: 0.5471 | Val Acc: 0.5493\n",
      "Epoch 290/1000 | Loss: 1.6158 | Train Acc: 0.5474 | Val Acc: 0.5503\n",
      "Epoch 291/1000 | Loss: 1.6147 | Train Acc: 0.5479 | Val Acc: 0.5505\n",
      "Epoch 292/1000 | Loss: 1.6136 | Train Acc: 0.5483 | Val Acc: 0.5510\n",
      "Epoch 293/1000 | Loss: 1.6124 | Train Acc: 0.5489 | Val Acc: 0.5512\n",
      "Epoch 294/1000 | Loss: 1.6113 | Train Acc: 0.5493 | Val Acc: 0.5517\n",
      "Epoch 295/1000 | Loss: 1.6101 | Train Acc: 0.5498 | Val Acc: 0.5523\n",
      "Epoch 296/1000 | Loss: 1.6090 | Train Acc: 0.5501 | Val Acc: 0.5535\n",
      "Epoch 297/1000 | Loss: 1.6079 | Train Acc: 0.5504 | Val Acc: 0.5543\n",
      "Epoch 298/1000 | Loss: 1.6068 | Train Acc: 0.5507 | Val Acc: 0.5543\n",
      "Epoch 299/1000 | Loss: 1.6056 | Train Acc: 0.5511 | Val Acc: 0.5543\n",
      "Epoch 300/1000 | Loss: 1.6045 | Train Acc: 0.5515 | Val Acc: 0.5548\n",
      "Epoch 301/1000 | Loss: 1.6034 | Train Acc: 0.5518 | Val Acc: 0.5547\n",
      "Epoch 302/1000 | Loss: 1.6023 | Train Acc: 0.5521 | Val Acc: 0.5555\n",
      "Epoch 303/1000 | Loss: 1.6012 | Train Acc: 0.5526 | Val Acc: 0.5555\n",
      "Epoch 304/1000 | Loss: 1.6001 | Train Acc: 0.5529 | Val Acc: 0.5557\n",
      "Epoch 305/1000 | Loss: 1.5990 | Train Acc: 0.5531 | Val Acc: 0.5557\n",
      "Epoch 306/1000 | Loss: 1.5979 | Train Acc: 0.5534 | Val Acc: 0.5558\n",
      "Epoch 307/1000 | Loss: 1.5967 | Train Acc: 0.5537 | Val Acc: 0.5560\n",
      "Epoch 308/1000 | Loss: 1.5957 | Train Acc: 0.5541 | Val Acc: 0.5563\n",
      "Epoch 309/1000 | Loss: 1.5946 | Train Acc: 0.5544 | Val Acc: 0.5560\n",
      "Epoch 310/1000 | Loss: 1.5935 | Train Acc: 0.5548 | Val Acc: 0.5568\n",
      "Epoch 311/1000 | Loss: 1.5924 | Train Acc: 0.5550 | Val Acc: 0.5572\n",
      "Epoch 312/1000 | Loss: 1.5913 | Train Acc: 0.5553 | Val Acc: 0.5577\n",
      "Epoch 313/1000 | Loss: 1.5902 | Train Acc: 0.5556 | Val Acc: 0.5582\n",
      "Epoch 314/1000 | Loss: 1.5892 | Train Acc: 0.5559 | Val Acc: 0.5583\n",
      "Epoch 315/1000 | Loss: 1.5881 | Train Acc: 0.5562 | Val Acc: 0.5587\n",
      "Epoch 316/1000 | Loss: 1.5870 | Train Acc: 0.5564 | Val Acc: 0.5590\n",
      "Epoch 317/1000 | Loss: 1.5860 | Train Acc: 0.5569 | Val Acc: 0.5593\n",
      "Epoch 318/1000 | Loss: 1.5849 | Train Acc: 0.5573 | Val Acc: 0.5595\n",
      "Epoch 319/1000 | Loss: 1.5838 | Train Acc: 0.5578 | Val Acc: 0.5597\n",
      "Epoch 320/1000 | Loss: 1.5828 | Train Acc: 0.5579 | Val Acc: 0.5597\n",
      "Epoch 321/1000 | Loss: 1.5817 | Train Acc: 0.5585 | Val Acc: 0.5602\n",
      "Epoch 322/1000 | Loss: 1.5807 | Train Acc: 0.5589 | Val Acc: 0.5605\n",
      "Epoch 323/1000 | Loss: 1.5796 | Train Acc: 0.5593 | Val Acc: 0.5607\n",
      "Epoch 324/1000 | Loss: 1.5786 | Train Acc: 0.5597 | Val Acc: 0.5612\n",
      "Epoch 325/1000 | Loss: 1.5775 | Train Acc: 0.5598 | Val Acc: 0.5608\n",
      "Epoch 326/1000 | Loss: 1.5765 | Train Acc: 0.5600 | Val Acc: 0.5618\n",
      "Epoch 327/1000 | Loss: 1.5755 | Train Acc: 0.5602 | Val Acc: 0.5618\n",
      "Epoch 328/1000 | Loss: 1.5744 | Train Acc: 0.5606 | Val Acc: 0.5623\n",
      "Epoch 329/1000 | Loss: 1.5734 | Train Acc: 0.5609 | Val Acc: 0.5625\n",
      "Epoch 330/1000 | Loss: 1.5724 | Train Acc: 0.5611 | Val Acc: 0.5623\n",
      "Epoch 331/1000 | Loss: 1.5713 | Train Acc: 0.5615 | Val Acc: 0.5632\n",
      "Epoch 332/1000 | Loss: 1.5703 | Train Acc: 0.5619 | Val Acc: 0.5632\n",
      "Epoch 333/1000 | Loss: 1.5693 | Train Acc: 0.5620 | Val Acc: 0.5635\n",
      "Epoch 334/1000 | Loss: 1.5683 | Train Acc: 0.5624 | Val Acc: 0.5642\n",
      "Epoch 335/1000 | Loss: 1.5673 | Train Acc: 0.5627 | Val Acc: 0.5648\n",
      "Epoch 336/1000 | Loss: 1.5663 | Train Acc: 0.5629 | Val Acc: 0.5653\n",
      "Epoch 337/1000 | Loss: 1.5653 | Train Acc: 0.5632 | Val Acc: 0.5658\n",
      "Epoch 338/1000 | Loss: 1.5643 | Train Acc: 0.5632 | Val Acc: 0.5662\n",
      "Epoch 339/1000 | Loss: 1.5633 | Train Acc: 0.5636 | Val Acc: 0.5667\n",
      "Epoch 340/1000 | Loss: 1.5623 | Train Acc: 0.5640 | Val Acc: 0.5665\n",
      "Epoch 341/1000 | Loss: 1.5613 | Train Acc: 0.5643 | Val Acc: 0.5670\n",
      "Epoch 342/1000 | Loss: 1.5603 | Train Acc: 0.5645 | Val Acc: 0.5672\n",
      "Epoch 343/1000 | Loss: 1.5593 | Train Acc: 0.5646 | Val Acc: 0.5672\n",
      "Epoch 344/1000 | Loss: 1.5583 | Train Acc: 0.5647 | Val Acc: 0.5673\n",
      "Epoch 345/1000 | Loss: 1.5573 | Train Acc: 0.5650 | Val Acc: 0.5673\n",
      "Epoch 346/1000 | Loss: 1.5563 | Train Acc: 0.5652 | Val Acc: 0.5677\n",
      "Epoch 347/1000 | Loss: 1.5553 | Train Acc: 0.5654 | Val Acc: 0.5680\n",
      "Epoch 348/1000 | Loss: 1.5544 | Train Acc: 0.5657 | Val Acc: 0.5680\n",
      "Epoch 349/1000 | Loss: 1.5534 | Train Acc: 0.5661 | Val Acc: 0.5683\n",
      "Epoch 350/1000 | Loss: 1.5524 | Train Acc: 0.5666 | Val Acc: 0.5685\n",
      "Epoch 351/1000 | Loss: 1.5514 | Train Acc: 0.5670 | Val Acc: 0.5688\n",
      "Epoch 352/1000 | Loss: 1.5505 | Train Acc: 0.5672 | Val Acc: 0.5687\n",
      "Epoch 353/1000 | Loss: 1.5495 | Train Acc: 0.5673 | Val Acc: 0.5688\n",
      "Epoch 354/1000 | Loss: 1.5486 | Train Acc: 0.5678 | Val Acc: 0.5690\n",
      "Epoch 355/1000 | Loss: 1.5476 | Train Acc: 0.5681 | Val Acc: 0.5692\n",
      "Epoch 356/1000 | Loss: 1.5466 | Train Acc: 0.5683 | Val Acc: 0.5693\n",
      "Epoch 357/1000 | Loss: 1.5457 | Train Acc: 0.5687 | Val Acc: 0.5693\n",
      "Epoch 358/1000 | Loss: 1.5447 | Train Acc: 0.5691 | Val Acc: 0.5700\n",
      "Epoch 359/1000 | Loss: 1.5438 | Train Acc: 0.5694 | Val Acc: 0.5702\n",
      "Epoch 360/1000 | Loss: 1.5428 | Train Acc: 0.5696 | Val Acc: 0.5707\n",
      "Epoch 361/1000 | Loss: 1.5419 | Train Acc: 0.5699 | Val Acc: 0.5707\n",
      "Epoch 362/1000 | Loss: 1.5410 | Train Acc: 0.5701 | Val Acc: 0.5710\n",
      "Epoch 363/1000 | Loss: 1.5400 | Train Acc: 0.5703 | Val Acc: 0.5710\n",
      "Epoch 364/1000 | Loss: 1.5391 | Train Acc: 0.5706 | Val Acc: 0.5710\n",
      "Epoch 365/1000 | Loss: 1.5381 | Train Acc: 0.5709 | Val Acc: 0.5712\n",
      "Epoch 366/1000 | Loss: 1.5372 | Train Acc: 0.5711 | Val Acc: 0.5717\n",
      "Epoch 367/1000 | Loss: 1.5363 | Train Acc: 0.5715 | Val Acc: 0.5717\n",
      "Epoch 368/1000 | Loss: 1.5353 | Train Acc: 0.5719 | Val Acc: 0.5722\n",
      "Epoch 369/1000 | Loss: 1.5344 | Train Acc: 0.5721 | Val Acc: 0.5733\n",
      "Epoch 370/1000 | Loss: 1.5335 | Train Acc: 0.5723 | Val Acc: 0.5737\n",
      "Epoch 371/1000 | Loss: 1.5326 | Train Acc: 0.5726 | Val Acc: 0.5738\n",
      "Epoch 372/1000 | Loss: 1.5316 | Train Acc: 0.5729 | Val Acc: 0.5742\n",
      "Epoch 373/1000 | Loss: 1.5307 | Train Acc: 0.5732 | Val Acc: 0.5742\n",
      "Epoch 374/1000 | Loss: 1.5298 | Train Acc: 0.5735 | Val Acc: 0.5747\n",
      "Epoch 375/1000 | Loss: 1.5289 | Train Acc: 0.5738 | Val Acc: 0.5748\n",
      "Epoch 376/1000 | Loss: 1.5280 | Train Acc: 0.5739 | Val Acc: 0.5750\n",
      "Epoch 377/1000 | Loss: 1.5271 | Train Acc: 0.5742 | Val Acc: 0.5753\n",
      "Epoch 378/1000 | Loss: 1.5262 | Train Acc: 0.5744 | Val Acc: 0.5755\n",
      "Epoch 379/1000 | Loss: 1.5253 | Train Acc: 0.5746 | Val Acc: 0.5753\n",
      "Epoch 380/1000 | Loss: 1.5244 | Train Acc: 0.5749 | Val Acc: 0.5753\n",
      "Epoch 381/1000 | Loss: 1.5235 | Train Acc: 0.5753 | Val Acc: 0.5758\n",
      "Epoch 382/1000 | Loss: 1.5226 | Train Acc: 0.5754 | Val Acc: 0.5758\n",
      "Epoch 383/1000 | Loss: 1.5217 | Train Acc: 0.5757 | Val Acc: 0.5758\n",
      "Epoch 384/1000 | Loss: 1.5208 | Train Acc: 0.5759 | Val Acc: 0.5763\n",
      "Epoch 385/1000 | Loss: 1.5199 | Train Acc: 0.5762 | Val Acc: 0.5763\n",
      "Epoch 386/1000 | Loss: 1.5190 | Train Acc: 0.5764 | Val Acc: 0.5767\n",
      "Epoch 387/1000 | Loss: 1.5181 | Train Acc: 0.5768 | Val Acc: 0.5767\n",
      "Epoch 388/1000 | Loss: 1.5173 | Train Acc: 0.5770 | Val Acc: 0.5768\n",
      "Epoch 389/1000 | Loss: 1.5164 | Train Acc: 0.5772 | Val Acc: 0.5773\n",
      "Epoch 390/1000 | Loss: 1.5155 | Train Acc: 0.5775 | Val Acc: 0.5778\n",
      "Epoch 391/1000 | Loss: 1.5146 | Train Acc: 0.5777 | Val Acc: 0.5785\n",
      "Epoch 392/1000 | Loss: 1.5138 | Train Acc: 0.5779 | Val Acc: 0.5792\n",
      "Epoch 393/1000 | Loss: 1.5129 | Train Acc: 0.5780 | Val Acc: 0.5793\n",
      "Epoch 394/1000 | Loss: 1.5120 | Train Acc: 0.5782 | Val Acc: 0.5798\n",
      "Epoch 395/1000 | Loss: 1.5112 | Train Acc: 0.5786 | Val Acc: 0.5803\n",
      "Epoch 396/1000 | Loss: 1.5103 | Train Acc: 0.5790 | Val Acc: 0.5800\n",
      "Epoch 397/1000 | Loss: 1.5094 | Train Acc: 0.5792 | Val Acc: 0.5802\n",
      "Epoch 398/1000 | Loss: 1.5086 | Train Acc: 0.5794 | Val Acc: 0.5807\n",
      "Epoch 399/1000 | Loss: 1.5077 | Train Acc: 0.5795 | Val Acc: 0.5808\n",
      "Epoch 400/1000 | Loss: 1.5068 | Train Acc: 0.5798 | Val Acc: 0.5812\n",
      "Epoch 401/1000 | Loss: 1.5060 | Train Acc: 0.5800 | Val Acc: 0.5813\n",
      "Epoch 402/1000 | Loss: 1.5051 | Train Acc: 0.5801 | Val Acc: 0.5817\n",
      "Epoch 403/1000 | Loss: 1.5043 | Train Acc: 0.5802 | Val Acc: 0.5817\n",
      "Epoch 404/1000 | Loss: 1.5035 | Train Acc: 0.5804 | Val Acc: 0.5820\n",
      "Epoch 405/1000 | Loss: 1.5026 | Train Acc: 0.5807 | Val Acc: 0.5823\n",
      "Epoch 406/1000 | Loss: 1.5018 | Train Acc: 0.5810 | Val Acc: 0.5828\n",
      "Epoch 407/1000 | Loss: 1.5009 | Train Acc: 0.5813 | Val Acc: 0.5830\n",
      "Epoch 408/1000 | Loss: 1.5001 | Train Acc: 0.5813 | Val Acc: 0.5838\n",
      "Epoch 409/1000 | Loss: 1.4993 | Train Acc: 0.5815 | Val Acc: 0.5842\n",
      "Epoch 410/1000 | Loss: 1.4984 | Train Acc: 0.5819 | Val Acc: 0.5842\n",
      "Epoch 411/1000 | Loss: 1.4976 | Train Acc: 0.5821 | Val Acc: 0.5845\n",
      "Epoch 412/1000 | Loss: 1.4968 | Train Acc: 0.5824 | Val Acc: 0.5848\n",
      "Epoch 413/1000 | Loss: 1.4959 | Train Acc: 0.5827 | Val Acc: 0.5852\n",
      "Epoch 414/1000 | Loss: 1.4951 | Train Acc: 0.5831 | Val Acc: 0.5852\n",
      "Epoch 415/1000 | Loss: 1.4943 | Train Acc: 0.5832 | Val Acc: 0.5855\n",
      "Epoch 416/1000 | Loss: 1.4935 | Train Acc: 0.5834 | Val Acc: 0.5855\n",
      "Epoch 417/1000 | Loss: 1.4926 | Train Acc: 0.5837 | Val Acc: 0.5855\n",
      "Epoch 418/1000 | Loss: 1.4918 | Train Acc: 0.5839 | Val Acc: 0.5857\n",
      "Epoch 419/1000 | Loss: 1.4910 | Train Acc: 0.5842 | Val Acc: 0.5858\n",
      "Epoch 420/1000 | Loss: 1.4902 | Train Acc: 0.5844 | Val Acc: 0.5863\n",
      "Epoch 421/1000 | Loss: 1.4894 | Train Acc: 0.5845 | Val Acc: 0.5862\n",
      "Epoch 422/1000 | Loss: 1.4886 | Train Acc: 0.5847 | Val Acc: 0.5862\n",
      "Epoch 423/1000 | Loss: 1.4878 | Train Acc: 0.5852 | Val Acc: 0.5865\n",
      "Epoch 424/1000 | Loss: 1.4870 | Train Acc: 0.5854 | Val Acc: 0.5870\n",
      "Epoch 425/1000 | Loss: 1.4861 | Train Acc: 0.5855 | Val Acc: 0.5877\n",
      "Epoch 426/1000 | Loss: 1.4854 | Train Acc: 0.5857 | Val Acc: 0.5878\n",
      "Epoch 427/1000 | Loss: 1.4846 | Train Acc: 0.5860 | Val Acc: 0.5878\n",
      "Epoch 428/1000 | Loss: 1.4837 | Train Acc: 0.5863 | Val Acc: 0.5882\n",
      "Epoch 429/1000 | Loss: 1.4830 | Train Acc: 0.5864 | Val Acc: 0.5880\n",
      "Epoch 430/1000 | Loss: 1.4822 | Train Acc: 0.5866 | Val Acc: 0.5882\n",
      "Epoch 431/1000 | Loss: 1.4814 | Train Acc: 0.5868 | Val Acc: 0.5885\n",
      "Epoch 432/1000 | Loss: 1.4806 | Train Acc: 0.5869 | Val Acc: 0.5888\n",
      "Epoch 433/1000 | Loss: 1.4798 | Train Acc: 0.5872 | Val Acc: 0.5890\n",
      "Epoch 434/1000 | Loss: 1.4790 | Train Acc: 0.5875 | Val Acc: 0.5887\n",
      "Epoch 435/1000 | Loss: 1.4782 | Train Acc: 0.5876 | Val Acc: 0.5885\n",
      "Epoch 436/1000 | Loss: 1.4774 | Train Acc: 0.5877 | Val Acc: 0.5887\n",
      "Epoch 437/1000 | Loss: 1.4766 | Train Acc: 0.5878 | Val Acc: 0.5890\n",
      "Epoch 438/1000 | Loss: 1.4759 | Train Acc: 0.5879 | Val Acc: 0.5893\n",
      "Epoch 439/1000 | Loss: 1.4751 | Train Acc: 0.5881 | Val Acc: 0.5893\n",
      "Epoch 440/1000 | Loss: 1.4743 | Train Acc: 0.5882 | Val Acc: 0.5895\n",
      "Epoch 441/1000 | Loss: 1.4735 | Train Acc: 0.5885 | Val Acc: 0.5900\n",
      "Epoch 442/1000 | Loss: 1.4728 | Train Acc: 0.5887 | Val Acc: 0.5898\n",
      "Epoch 443/1000 | Loss: 1.4720 | Train Acc: 0.5888 | Val Acc: 0.5902\n",
      "Epoch 444/1000 | Loss: 1.4712 | Train Acc: 0.5890 | Val Acc: 0.5900\n",
      "Epoch 445/1000 | Loss: 1.4705 | Train Acc: 0.5891 | Val Acc: 0.5903\n",
      "Epoch 446/1000 | Loss: 1.4697 | Train Acc: 0.5893 | Val Acc: 0.5907\n",
      "Epoch 447/1000 | Loss: 1.4690 | Train Acc: 0.5895 | Val Acc: 0.5908\n",
      "Epoch 448/1000 | Loss: 1.4682 | Train Acc: 0.5897 | Val Acc: 0.5908\n",
      "Epoch 449/1000 | Loss: 1.4674 | Train Acc: 0.5900 | Val Acc: 0.5910\n",
      "Epoch 450/1000 | Loss: 1.4667 | Train Acc: 0.5900 | Val Acc: 0.5910\n",
      "Epoch 451/1000 | Loss: 1.4659 | Train Acc: 0.5902 | Val Acc: 0.5913\n",
      "Epoch 452/1000 | Loss: 1.4651 | Train Acc: 0.5904 | Val Acc: 0.5913\n",
      "Epoch 453/1000 | Loss: 1.4644 | Train Acc: 0.5906 | Val Acc: 0.5917\n",
      "Epoch 454/1000 | Loss: 1.4636 | Train Acc: 0.5909 | Val Acc: 0.5917\n",
      "Epoch 455/1000 | Loss: 1.4629 | Train Acc: 0.5909 | Val Acc: 0.5918\n",
      "Epoch 456/1000 | Loss: 1.4622 | Train Acc: 0.5911 | Val Acc: 0.5918\n",
      "Epoch 457/1000 | Loss: 1.4614 | Train Acc: 0.5912 | Val Acc: 0.5920\n",
      "Epoch 458/1000 | Loss: 1.4607 | Train Acc: 0.5914 | Val Acc: 0.5918\n",
      "Epoch 459/1000 | Loss: 1.4599 | Train Acc: 0.5916 | Val Acc: 0.5923\n",
      "Epoch 460/1000 | Loss: 1.4592 | Train Acc: 0.5918 | Val Acc: 0.5925\n",
      "Epoch 461/1000 | Loss: 1.4584 | Train Acc: 0.5920 | Val Acc: 0.5928\n",
      "Epoch 462/1000 | Loss: 1.4577 | Train Acc: 0.5922 | Val Acc: 0.5933\n",
      "Epoch 463/1000 | Loss: 1.4570 | Train Acc: 0.5924 | Val Acc: 0.5935\n",
      "Epoch 464/1000 | Loss: 1.4562 | Train Acc: 0.5925 | Val Acc: 0.5937\n",
      "Epoch 465/1000 | Loss: 1.4555 | Train Acc: 0.5926 | Val Acc: 0.5942\n",
      "Epoch 466/1000 | Loss: 1.4548 | Train Acc: 0.5927 | Val Acc: 0.5943\n",
      "Epoch 467/1000 | Loss: 1.4540 | Train Acc: 0.5928 | Val Acc: 0.5950\n",
      "Epoch 468/1000 | Loss: 1.4533 | Train Acc: 0.5929 | Val Acc: 0.5952\n",
      "Epoch 469/1000 | Loss: 1.4526 | Train Acc: 0.5930 | Val Acc: 0.5950\n",
      "Epoch 470/1000 | Loss: 1.4519 | Train Acc: 0.5932 | Val Acc: 0.5952\n",
      "Epoch 471/1000 | Loss: 1.4512 | Train Acc: 0.5934 | Val Acc: 0.5953\n",
      "Epoch 472/1000 | Loss: 1.4504 | Train Acc: 0.5936 | Val Acc: 0.5958\n",
      "Epoch 473/1000 | Loss: 1.4497 | Train Acc: 0.5938 | Val Acc: 0.5958\n",
      "Epoch 474/1000 | Loss: 1.4490 | Train Acc: 0.5941 | Val Acc: 0.5960\n",
      "Epoch 475/1000 | Loss: 1.4483 | Train Acc: 0.5943 | Val Acc: 0.5965\n",
      "Epoch 476/1000 | Loss: 1.4476 | Train Acc: 0.5946 | Val Acc: 0.5965\n",
      "Epoch 477/1000 | Loss: 1.4468 | Train Acc: 0.5948 | Val Acc: 0.5968\n",
      "Epoch 478/1000 | Loss: 1.4461 | Train Acc: 0.5949 | Val Acc: 0.5968\n",
      "Epoch 479/1000 | Loss: 1.4454 | Train Acc: 0.5952 | Val Acc: 0.5968\n",
      "Epoch 480/1000 | Loss: 1.4447 | Train Acc: 0.5954 | Val Acc: 0.5973\n",
      "Epoch 481/1000 | Loss: 1.4440 | Train Acc: 0.5955 | Val Acc: 0.5973\n",
      "Epoch 482/1000 | Loss: 1.4433 | Train Acc: 0.5956 | Val Acc: 0.5972\n",
      "Epoch 483/1000 | Loss: 1.4426 | Train Acc: 0.5957 | Val Acc: 0.5973\n",
      "Epoch 484/1000 | Loss: 1.4419 | Train Acc: 0.5958 | Val Acc: 0.5977\n",
      "Epoch 485/1000 | Loss: 1.4412 | Train Acc: 0.5959 | Val Acc: 0.5980\n",
      "Epoch 486/1000 | Loss: 1.4405 | Train Acc: 0.5960 | Val Acc: 0.5980\n",
      "Epoch 487/1000 | Loss: 1.4398 | Train Acc: 0.5962 | Val Acc: 0.5983\n",
      "Epoch 488/1000 | Loss: 1.4391 | Train Acc: 0.5963 | Val Acc: 0.5982\n",
      "Epoch 489/1000 | Loss: 1.4384 | Train Acc: 0.5964 | Val Acc: 0.5982\n",
      "Epoch 490/1000 | Loss: 1.4377 | Train Acc: 0.5966 | Val Acc: 0.5980\n",
      "Epoch 491/1000 | Loss: 1.4370 | Train Acc: 0.5967 | Val Acc: 0.5980\n",
      "Epoch 492/1000 | Loss: 1.4363 | Train Acc: 0.5968 | Val Acc: 0.5980\n",
      "Epoch 493/1000 | Loss: 1.4356 | Train Acc: 0.5970 | Val Acc: 0.5983\n",
      "Epoch 494/1000 | Loss: 1.4350 | Train Acc: 0.5971 | Val Acc: 0.5983\n",
      "Epoch 495/1000 | Loss: 1.4343 | Train Acc: 0.5974 | Val Acc: 0.5983\n",
      "Epoch 496/1000 | Loss: 1.4336 | Train Acc: 0.5975 | Val Acc: 0.5983\n",
      "Epoch 497/1000 | Loss: 1.4329 | Train Acc: 0.5976 | Val Acc: 0.5985\n",
      "Epoch 498/1000 | Loss: 1.4322 | Train Acc: 0.5979 | Val Acc: 0.5985\n",
      "Epoch 499/1000 | Loss: 1.4316 | Train Acc: 0.5981 | Val Acc: 0.5985\n",
      "Epoch 500/1000 | Loss: 1.4309 | Train Acc: 0.5983 | Val Acc: 0.5983\n",
      "Epoch 501/1000 | Loss: 1.4302 | Train Acc: 0.5985 | Val Acc: 0.5988\n",
      "Epoch 502/1000 | Loss: 1.4295 | Train Acc: 0.5987 | Val Acc: 0.5988\n",
      "Epoch 503/1000 | Loss: 1.4289 | Train Acc: 0.5988 | Val Acc: 0.5988\n",
      "Epoch 504/1000 | Loss: 1.4282 | Train Acc: 0.5989 | Val Acc: 0.5988\n",
      "Epoch 505/1000 | Loss: 1.4275 | Train Acc: 0.5989 | Val Acc: 0.5990\n",
      "Epoch 506/1000 | Loss: 1.4269 | Train Acc: 0.5991 | Val Acc: 0.5992\n",
      "Epoch 507/1000 | Loss: 1.4262 | Train Acc: 0.5993 | Val Acc: 0.5993\n",
      "Epoch 508/1000 | Loss: 1.4255 | Train Acc: 0.5994 | Val Acc: 0.5990\n",
      "Epoch 509/1000 | Loss: 1.4249 | Train Acc: 0.5996 | Val Acc: 0.5995\n",
      "Epoch 510/1000 | Loss: 1.4242 | Train Acc: 0.5996 | Val Acc: 0.5995\n",
      "Epoch 511/1000 | Loss: 1.4235 | Train Acc: 0.5998 | Val Acc: 0.5997\n",
      "Epoch 512/1000 | Loss: 1.4229 | Train Acc: 0.5998 | Val Acc: 0.5998\n",
      "Epoch 513/1000 | Loss: 1.4222 | Train Acc: 0.5999 | Val Acc: 0.5998\n",
      "Epoch 514/1000 | Loss: 1.4216 | Train Acc: 0.6002 | Val Acc: 0.5997\n",
      "Epoch 515/1000 | Loss: 1.4209 | Train Acc: 0.6003 | Val Acc: 0.5997\n",
      "Epoch 516/1000 | Loss: 1.4203 | Train Acc: 0.6005 | Val Acc: 0.6000\n",
      "Epoch 517/1000 | Loss: 1.4196 | Train Acc: 0.6006 | Val Acc: 0.6002\n",
      "Epoch 518/1000 | Loss: 1.4189 | Train Acc: 0.6007 | Val Acc: 0.6005\n",
      "Epoch 519/1000 | Loss: 1.4183 | Train Acc: 0.6008 | Val Acc: 0.6005\n",
      "Epoch 520/1000 | Loss: 1.4177 | Train Acc: 0.6009 | Val Acc: 0.6005\n",
      "Epoch 521/1000 | Loss: 1.4170 | Train Acc: 0.6010 | Val Acc: 0.6008\n",
      "Epoch 522/1000 | Loss: 1.4163 | Train Acc: 0.6011 | Val Acc: 0.6013\n",
      "Epoch 523/1000 | Loss: 1.4157 | Train Acc: 0.6013 | Val Acc: 0.6015\n",
      "Epoch 524/1000 | Loss: 1.4151 | Train Acc: 0.6015 | Val Acc: 0.6015\n",
      "Epoch 525/1000 | Loss: 1.4145 | Train Acc: 0.6017 | Val Acc: 0.6017\n",
      "Epoch 526/1000 | Loss: 1.4138 | Train Acc: 0.6018 | Val Acc: 0.6017\n",
      "Epoch 527/1000 | Loss: 1.4131 | Train Acc: 0.6019 | Val Acc: 0.6017\n",
      "Epoch 528/1000 | Loss: 1.4125 | Train Acc: 0.6019 | Val Acc: 0.6018\n",
      "Epoch 529/1000 | Loss: 1.4118 | Train Acc: 0.6021 | Val Acc: 0.6022\n",
      "Epoch 530/1000 | Loss: 1.4113 | Train Acc: 0.6022 | Val Acc: 0.6025\n",
      "Epoch 531/1000 | Loss: 1.4106 | Train Acc: 0.6024 | Val Acc: 0.6027\n",
      "Epoch 532/1000 | Loss: 1.4100 | Train Acc: 0.6025 | Val Acc: 0.6030\n",
      "Epoch 533/1000 | Loss: 1.4094 | Train Acc: 0.6026 | Val Acc: 0.6033\n",
      "Epoch 534/1000 | Loss: 1.4087 | Train Acc: 0.6028 | Val Acc: 0.6035\n",
      "Epoch 535/1000 | Loss: 1.4081 | Train Acc: 0.6030 | Val Acc: 0.6037\n",
      "Epoch 536/1000 | Loss: 1.4075 | Train Acc: 0.6030 | Val Acc: 0.6037\n",
      "Epoch 537/1000 | Loss: 1.4068 | Train Acc: 0.6032 | Val Acc: 0.6040\n",
      "Epoch 538/1000 | Loss: 1.4062 | Train Acc: 0.6033 | Val Acc: 0.6040\n",
      "Epoch 539/1000 | Loss: 1.4056 | Train Acc: 0.6034 | Val Acc: 0.6043\n",
      "Epoch 540/1000 | Loss: 1.4050 | Train Acc: 0.6036 | Val Acc: 0.6045\n",
      "Epoch 541/1000 | Loss: 1.4043 | Train Acc: 0.6037 | Val Acc: 0.6048\n",
      "Epoch 542/1000 | Loss: 1.4037 | Train Acc: 0.6040 | Val Acc: 0.6050\n",
      "Epoch 543/1000 | Loss: 1.4031 | Train Acc: 0.6041 | Val Acc: 0.6050\n",
      "Epoch 544/1000 | Loss: 1.4025 | Train Acc: 0.6041 | Val Acc: 0.6053\n",
      "Epoch 545/1000 | Loss: 1.4019 | Train Acc: 0.6044 | Val Acc: 0.6053\n",
      "Epoch 546/1000 | Loss: 1.4013 | Train Acc: 0.6045 | Val Acc: 0.6052\n",
      "Epoch 547/1000 | Loss: 1.4007 | Train Acc: 0.6047 | Val Acc: 0.6052\n",
      "Epoch 548/1000 | Loss: 1.4001 | Train Acc: 0.6049 | Val Acc: 0.6052\n",
      "Epoch 549/1000 | Loss: 1.3994 | Train Acc: 0.6051 | Val Acc: 0.6055\n",
      "Epoch 550/1000 | Loss: 1.3988 | Train Acc: 0.6052 | Val Acc: 0.6060\n",
      "Epoch 551/1000 | Loss: 1.3982 | Train Acc: 0.6054 | Val Acc: 0.6063\n",
      "Epoch 552/1000 | Loss: 1.3976 | Train Acc: 0.6056 | Val Acc: 0.6062\n",
      "Epoch 553/1000 | Loss: 1.3970 | Train Acc: 0.6058 | Val Acc: 0.6063\n",
      "Epoch 554/1000 | Loss: 1.3964 | Train Acc: 0.6060 | Val Acc: 0.6065\n",
      "Epoch 555/1000 | Loss: 1.3958 | Train Acc: 0.6061 | Val Acc: 0.6065\n",
      "Epoch 556/1000 | Loss: 1.3952 | Train Acc: 0.6062 | Val Acc: 0.6067\n",
      "Epoch 557/1000 | Loss: 1.3946 | Train Acc: 0.6064 | Val Acc: 0.6068\n",
      "Epoch 558/1000 | Loss: 1.3940 | Train Acc: 0.6065 | Val Acc: 0.6068\n",
      "Epoch 559/1000 | Loss: 1.3934 | Train Acc: 0.6065 | Val Acc: 0.6068\n",
      "Epoch 560/1000 | Loss: 1.3928 | Train Acc: 0.6067 | Val Acc: 0.6068\n",
      "Epoch 561/1000 | Loss: 1.3922 | Train Acc: 0.6068 | Val Acc: 0.6072\n",
      "Epoch 562/1000 | Loss: 1.3916 | Train Acc: 0.6068 | Val Acc: 0.6070\n",
      "Epoch 563/1000 | Loss: 1.3910 | Train Acc: 0.6070 | Val Acc: 0.6073\n",
      "Epoch 564/1000 | Loss: 1.3904 | Train Acc: 0.6072 | Val Acc: 0.6073\n",
      "Epoch 565/1000 | Loss: 1.3898 | Train Acc: 0.6074 | Val Acc: 0.6075\n",
      "Epoch 566/1000 | Loss: 1.3892 | Train Acc: 0.6076 | Val Acc: 0.6082\n",
      "Epoch 567/1000 | Loss: 1.3887 | Train Acc: 0.6078 | Val Acc: 0.6087\n",
      "Epoch 568/1000 | Loss: 1.3881 | Train Acc: 0.6081 | Val Acc: 0.6090\n",
      "Epoch 569/1000 | Loss: 1.3875 | Train Acc: 0.6081 | Val Acc: 0.6088\n",
      "Epoch 570/1000 | Loss: 1.3869 | Train Acc: 0.6084 | Val Acc: 0.6088\n",
      "Epoch 571/1000 | Loss: 1.3863 | Train Acc: 0.6085 | Val Acc: 0.6090\n",
      "Epoch 572/1000 | Loss: 1.3857 | Train Acc: 0.6087 | Val Acc: 0.6093\n",
      "Epoch 573/1000 | Loss: 1.3852 | Train Acc: 0.6089 | Val Acc: 0.6097\n",
      "Epoch 574/1000 | Loss: 1.3846 | Train Acc: 0.6091 | Val Acc: 0.6097\n",
      "Epoch 575/1000 | Loss: 1.3840 | Train Acc: 0.6092 | Val Acc: 0.6097\n",
      "Epoch 576/1000 | Loss: 1.3834 | Train Acc: 0.6094 | Val Acc: 0.6098\n",
      "Epoch 577/1000 | Loss: 1.3828 | Train Acc: 0.6095 | Val Acc: 0.6100\n",
      "Epoch 578/1000 | Loss: 1.3822 | Train Acc: 0.6098 | Val Acc: 0.6100\n",
      "Epoch 579/1000 | Loss: 1.3817 | Train Acc: 0.6099 | Val Acc: 0.6103\n",
      "Epoch 580/1000 | Loss: 1.3811 | Train Acc: 0.6101 | Val Acc: 0.6107\n",
      "Epoch 581/1000 | Loss: 1.3805 | Train Acc: 0.6102 | Val Acc: 0.6105\n",
      "Epoch 582/1000 | Loss: 1.3800 | Train Acc: 0.6103 | Val Acc: 0.6107\n",
      "Epoch 583/1000 | Loss: 1.3794 | Train Acc: 0.6104 | Val Acc: 0.6105\n",
      "Epoch 584/1000 | Loss: 1.3788 | Train Acc: 0.6106 | Val Acc: 0.6107\n",
      "Epoch 585/1000 | Loss: 1.3783 | Train Acc: 0.6107 | Val Acc: 0.6108\n",
      "Epoch 586/1000 | Loss: 1.3777 | Train Acc: 0.6108 | Val Acc: 0.6108\n",
      "Epoch 587/1000 | Loss: 1.3771 | Train Acc: 0.6108 | Val Acc: 0.6110\n",
      "Epoch 588/1000 | Loss: 1.3766 | Train Acc: 0.6109 | Val Acc: 0.6112\n",
      "Epoch 589/1000 | Loss: 1.3760 | Train Acc: 0.6111 | Val Acc: 0.6115\n",
      "Epoch 590/1000 | Loss: 1.3755 | Train Acc: 0.6113 | Val Acc: 0.6112\n",
      "Epoch 591/1000 | Loss: 1.3749 | Train Acc: 0.6114 | Val Acc: 0.6115\n",
      "Epoch 592/1000 | Loss: 1.3743 | Train Acc: 0.6115 | Val Acc: 0.6117\n",
      "Epoch 593/1000 | Loss: 1.3738 | Train Acc: 0.6115 | Val Acc: 0.6122\n",
      "Epoch 594/1000 | Loss: 1.3732 | Train Acc: 0.6116 | Val Acc: 0.6122\n",
      "Epoch 595/1000 | Loss: 1.3727 | Train Acc: 0.6117 | Val Acc: 0.6123\n",
      "Epoch 596/1000 | Loss: 1.3721 | Train Acc: 0.6120 | Val Acc: 0.6128\n",
      "Epoch 597/1000 | Loss: 1.3715 | Train Acc: 0.6121 | Val Acc: 0.6130\n",
      "Epoch 598/1000 | Loss: 1.3710 | Train Acc: 0.6123 | Val Acc: 0.6135\n",
      "Epoch 599/1000 | Loss: 1.3704 | Train Acc: 0.6125 | Val Acc: 0.6135\n",
      "Epoch 600/1000 | Loss: 1.3699 | Train Acc: 0.6126 | Val Acc: 0.6137\n",
      "Epoch 601/1000 | Loss: 1.3693 | Train Acc: 0.6128 | Val Acc: 0.6137\n",
      "Epoch 602/1000 | Loss: 1.3688 | Train Acc: 0.6129 | Val Acc: 0.6138\n",
      "Epoch 603/1000 | Loss: 1.3683 | Train Acc: 0.6129 | Val Acc: 0.6142\n",
      "Epoch 604/1000 | Loss: 1.3677 | Train Acc: 0.6130 | Val Acc: 0.6142\n",
      "Epoch 605/1000 | Loss: 1.3672 | Train Acc: 0.6130 | Val Acc: 0.6145\n",
      "Epoch 606/1000 | Loss: 1.3666 | Train Acc: 0.6131 | Val Acc: 0.6147\n",
      "Epoch 607/1000 | Loss: 1.3661 | Train Acc: 0.6132 | Val Acc: 0.6148\n",
      "Epoch 608/1000 | Loss: 1.3656 | Train Acc: 0.6134 | Val Acc: 0.6152\n",
      "Epoch 609/1000 | Loss: 1.3650 | Train Acc: 0.6135 | Val Acc: 0.6155\n",
      "Epoch 610/1000 | Loss: 1.3644 | Train Acc: 0.6137 | Val Acc: 0.6155\n",
      "Epoch 611/1000 | Loss: 1.3639 | Train Acc: 0.6137 | Val Acc: 0.6158\n",
      "Epoch 612/1000 | Loss: 1.3634 | Train Acc: 0.6139 | Val Acc: 0.6158\n",
      "Epoch 613/1000 | Loss: 1.3628 | Train Acc: 0.6140 | Val Acc: 0.6158\n",
      "Epoch 614/1000 | Loss: 1.3623 | Train Acc: 0.6141 | Val Acc: 0.6160\n",
      "Epoch 615/1000 | Loss: 1.3617 | Train Acc: 0.6141 | Val Acc: 0.6162\n",
      "Epoch 616/1000 | Loss: 1.3612 | Train Acc: 0.6142 | Val Acc: 0.6162\n",
      "Epoch 617/1000 | Loss: 1.3607 | Train Acc: 0.6142 | Val Acc: 0.6162\n",
      "Epoch 618/1000 | Loss: 1.3602 | Train Acc: 0.6144 | Val Acc: 0.6163\n",
      "Epoch 619/1000 | Loss: 1.3596 | Train Acc: 0.6144 | Val Acc: 0.6165\n",
      "Epoch 620/1000 | Loss: 1.3591 | Train Acc: 0.6145 | Val Acc: 0.6167\n",
      "Epoch 621/1000 | Loss: 1.3586 | Train Acc: 0.6146 | Val Acc: 0.6167\n",
      "Epoch 622/1000 | Loss: 1.3580 | Train Acc: 0.6146 | Val Acc: 0.6170\n",
      "Epoch 623/1000 | Loss: 1.3575 | Train Acc: 0.6147 | Val Acc: 0.6172\n",
      "Epoch 624/1000 | Loss: 1.3570 | Train Acc: 0.6148 | Val Acc: 0.6172\n",
      "Epoch 625/1000 | Loss: 1.3565 | Train Acc: 0.6149 | Val Acc: 0.6172\n",
      "Epoch 626/1000 | Loss: 1.3559 | Train Acc: 0.6150 | Val Acc: 0.6170\n",
      "Epoch 627/1000 | Loss: 1.3554 | Train Acc: 0.6151 | Val Acc: 0.6173\n",
      "Epoch 628/1000 | Loss: 1.3549 | Train Acc: 0.6152 | Val Acc: 0.6173\n",
      "Epoch 629/1000 | Loss: 1.3544 | Train Acc: 0.6154 | Val Acc: 0.6175\n",
      "Epoch 630/1000 | Loss: 1.3539 | Train Acc: 0.6156 | Val Acc: 0.6175\n",
      "Epoch 631/1000 | Loss: 1.3533 | Train Acc: 0.6157 | Val Acc: 0.6178\n",
      "Epoch 632/1000 | Loss: 1.3528 | Train Acc: 0.6159 | Val Acc: 0.6180\n",
      "Epoch 633/1000 | Loss: 1.3523 | Train Acc: 0.6161 | Val Acc: 0.6180\n",
      "Epoch 634/1000 | Loss: 1.3518 | Train Acc: 0.6162 | Val Acc: 0.6182\n",
      "Epoch 635/1000 | Loss: 1.3513 | Train Acc: 0.6163 | Val Acc: 0.6182\n",
      "Epoch 636/1000 | Loss: 1.3508 | Train Acc: 0.6164 | Val Acc: 0.6182\n",
      "Epoch 637/1000 | Loss: 1.3502 | Train Acc: 0.6165 | Val Acc: 0.6182\n",
      "Epoch 638/1000 | Loss: 1.3497 | Train Acc: 0.6165 | Val Acc: 0.6182\n",
      "Epoch 639/1000 | Loss: 1.3492 | Train Acc: 0.6166 | Val Acc: 0.6182\n",
      "Epoch 640/1000 | Loss: 1.3487 | Train Acc: 0.6168 | Val Acc: 0.6183\n",
      "Epoch 641/1000 | Loss: 1.3482 | Train Acc: 0.6169 | Val Acc: 0.6183\n",
      "Epoch 642/1000 | Loss: 1.3477 | Train Acc: 0.6170 | Val Acc: 0.6185\n",
      "Epoch 643/1000 | Loss: 1.3472 | Train Acc: 0.6171 | Val Acc: 0.6185\n",
      "Epoch 644/1000 | Loss: 1.3467 | Train Acc: 0.6171 | Val Acc: 0.6185\n",
      "Epoch 645/1000 | Loss: 1.3462 | Train Acc: 0.6173 | Val Acc: 0.6187\n",
      "Epoch 646/1000 | Loss: 1.3457 | Train Acc: 0.6174 | Val Acc: 0.6187\n",
      "Epoch 647/1000 | Loss: 1.3452 | Train Acc: 0.6174 | Val Acc: 0.6187\n",
      "Epoch 648/1000 | Loss: 1.3447 | Train Acc: 0.6175 | Val Acc: 0.6187\n",
      "Epoch 649/1000 | Loss: 1.3442 | Train Acc: 0.6177 | Val Acc: 0.6187\n",
      "Epoch 650/1000 | Loss: 1.3436 | Train Acc: 0.6178 | Val Acc: 0.6187\n",
      "Epoch 651/1000 | Loss: 1.3432 | Train Acc: 0.6180 | Val Acc: 0.6187\n",
      "Epoch 652/1000 | Loss: 1.3427 | Train Acc: 0.6182 | Val Acc: 0.6192\n",
      "Epoch 653/1000 | Loss: 1.3422 | Train Acc: 0.6183 | Val Acc: 0.6193\n",
      "Epoch 654/1000 | Loss: 1.3416 | Train Acc: 0.6184 | Val Acc: 0.6193\n",
      "Epoch 655/1000 | Loss: 1.3412 | Train Acc: 0.6185 | Val Acc: 0.6195\n",
      "Epoch 656/1000 | Loss: 1.3407 | Train Acc: 0.6186 | Val Acc: 0.6200\n",
      "Epoch 657/1000 | Loss: 1.3402 | Train Acc: 0.6188 | Val Acc: 0.6200\n",
      "Epoch 658/1000 | Loss: 1.3397 | Train Acc: 0.6188 | Val Acc: 0.6200\n",
      "Epoch 659/1000 | Loss: 1.3392 | Train Acc: 0.6189 | Val Acc: 0.6197\n",
      "Epoch 660/1000 | Loss: 1.3387 | Train Acc: 0.6190 | Val Acc: 0.6198\n",
      "Epoch 661/1000 | Loss: 1.3382 | Train Acc: 0.6190 | Val Acc: 0.6202\n",
      "Epoch 662/1000 | Loss: 1.3377 | Train Acc: 0.6191 | Val Acc: 0.6202\n",
      "Epoch 663/1000 | Loss: 1.3372 | Train Acc: 0.6193 | Val Acc: 0.6202\n",
      "Epoch 664/1000 | Loss: 1.3367 | Train Acc: 0.6194 | Val Acc: 0.6203\n",
      "Epoch 665/1000 | Loss: 1.3363 | Train Acc: 0.6196 | Val Acc: 0.6205\n",
      "Epoch 666/1000 | Loss: 1.3358 | Train Acc: 0.6198 | Val Acc: 0.6207\n",
      "Epoch 667/1000 | Loss: 1.3353 | Train Acc: 0.6199 | Val Acc: 0.6205\n",
      "Epoch 668/1000 | Loss: 1.3348 | Train Acc: 0.6200 | Val Acc: 0.6207\n",
      "Epoch 669/1000 | Loss: 1.3343 | Train Acc: 0.6200 | Val Acc: 0.6208\n",
      "Epoch 670/1000 | Loss: 1.3338 | Train Acc: 0.6201 | Val Acc: 0.6208\n",
      "Epoch 671/1000 | Loss: 1.3334 | Train Acc: 0.6201 | Val Acc: 0.6208\n",
      "Epoch 672/1000 | Loss: 1.3329 | Train Acc: 0.6202 | Val Acc: 0.6208\n",
      "Epoch 673/1000 | Loss: 1.3324 | Train Acc: 0.6203 | Val Acc: 0.6208\n",
      "Epoch 674/1000 | Loss: 1.3319 | Train Acc: 0.6203 | Val Acc: 0.6208\n",
      "Epoch 675/1000 | Loss: 1.3314 | Train Acc: 0.6205 | Val Acc: 0.6208\n",
      "Epoch 676/1000 | Loss: 1.3310 | Train Acc: 0.6205 | Val Acc: 0.6210\n",
      "Epoch 677/1000 | Loss: 1.3305 | Train Acc: 0.6206 | Val Acc: 0.6210\n",
      "Epoch 678/1000 | Loss: 1.3300 | Train Acc: 0.6207 | Val Acc: 0.6210\n",
      "Epoch 679/1000 | Loss: 1.3295 | Train Acc: 0.6208 | Val Acc: 0.6210\n",
      "Epoch 680/1000 | Loss: 1.3290 | Train Acc: 0.6210 | Val Acc: 0.6210\n",
      "Epoch 681/1000 | Loss: 1.3286 | Train Acc: 0.6212 | Val Acc: 0.6213\n",
      "Epoch 682/1000 | Loss: 1.3281 | Train Acc: 0.6214 | Val Acc: 0.6213\n",
      "Epoch 683/1000 | Loss: 1.3276 | Train Acc: 0.6214 | Val Acc: 0.6218\n",
      "Epoch 684/1000 | Loss: 1.3271 | Train Acc: 0.6216 | Val Acc: 0.6220\n",
      "Epoch 685/1000 | Loss: 1.3267 | Train Acc: 0.6216 | Val Acc: 0.6220\n",
      "Epoch 686/1000 | Loss: 1.3263 | Train Acc: 0.6218 | Val Acc: 0.6222\n",
      "Epoch 687/1000 | Loss: 1.3257 | Train Acc: 0.6219 | Val Acc: 0.6222\n",
      "Epoch 688/1000 | Loss: 1.3252 | Train Acc: 0.6220 | Val Acc: 0.6223\n",
      "Epoch 689/1000 | Loss: 1.3248 | Train Acc: 0.6221 | Val Acc: 0.6225\n",
      "Epoch 690/1000 | Loss: 1.3244 | Train Acc: 0.6223 | Val Acc: 0.6225\n",
      "Epoch 691/1000 | Loss: 1.3239 | Train Acc: 0.6223 | Val Acc: 0.6225\n",
      "Epoch 692/1000 | Loss: 1.3234 | Train Acc: 0.6225 | Val Acc: 0.6225\n",
      "Epoch 693/1000 | Loss: 1.3230 | Train Acc: 0.6226 | Val Acc: 0.6225\n",
      "Epoch 694/1000 | Loss: 1.3225 | Train Acc: 0.6227 | Val Acc: 0.6227\n",
      "Epoch 695/1000 | Loss: 1.3220 | Train Acc: 0.6228 | Val Acc: 0.6227\n",
      "Epoch 696/1000 | Loss: 1.3216 | Train Acc: 0.6229 | Val Acc: 0.6227\n",
      "Epoch 697/1000 | Loss: 1.3211 | Train Acc: 0.6231 | Val Acc: 0.6227\n",
      "Epoch 698/1000 | Loss: 1.3207 | Train Acc: 0.6232 | Val Acc: 0.6227\n",
      "Epoch 699/1000 | Loss: 1.3202 | Train Acc: 0.6232 | Val Acc: 0.6230\n",
      "Epoch 700/1000 | Loss: 1.3197 | Train Acc: 0.6234 | Val Acc: 0.6235\n",
      "Epoch 701/1000 | Loss: 1.3193 | Train Acc: 0.6235 | Val Acc: 0.6237\n",
      "Epoch 702/1000 | Loss: 1.3188 | Train Acc: 0.6236 | Val Acc: 0.6240\n",
      "Epoch 703/1000 | Loss: 1.3184 | Train Acc: 0.6238 | Val Acc: 0.6242\n",
      "Epoch 704/1000 | Loss: 1.3179 | Train Acc: 0.6239 | Val Acc: 0.6242\n",
      "Epoch 705/1000 | Loss: 1.3175 | Train Acc: 0.6239 | Val Acc: 0.6242\n",
      "Epoch 706/1000 | Loss: 1.3170 | Train Acc: 0.6240 | Val Acc: 0.6240\n",
      "Epoch 707/1000 | Loss: 1.3166 | Train Acc: 0.6241 | Val Acc: 0.6240\n",
      "Epoch 708/1000 | Loss: 1.3161 | Train Acc: 0.6242 | Val Acc: 0.6242\n",
      "Epoch 709/1000 | Loss: 1.3156 | Train Acc: 0.6243 | Val Acc: 0.6243\n",
      "Epoch 710/1000 | Loss: 1.3152 | Train Acc: 0.6244 | Val Acc: 0.6243\n",
      "Epoch 711/1000 | Loss: 1.3147 | Train Acc: 0.6246 | Val Acc: 0.6242\n",
      "Epoch 712/1000 | Loss: 1.3143 | Train Acc: 0.6247 | Val Acc: 0.6247\n",
      "Epoch 713/1000 | Loss: 1.3138 | Train Acc: 0.6248 | Val Acc: 0.6247\n",
      "Epoch 714/1000 | Loss: 1.3134 | Train Acc: 0.6249 | Val Acc: 0.6247\n",
      "Epoch 715/1000 | Loss: 1.3130 | Train Acc: 0.6250 | Val Acc: 0.6248\n",
      "Epoch 716/1000 | Loss: 1.3125 | Train Acc: 0.6251 | Val Acc: 0.6250\n",
      "Epoch 717/1000 | Loss: 1.3121 | Train Acc: 0.6251 | Val Acc: 0.6250\n",
      "Epoch 718/1000 | Loss: 1.3116 | Train Acc: 0.6252 | Val Acc: 0.6252\n",
      "Epoch 719/1000 | Loss: 1.3112 | Train Acc: 0.6253 | Val Acc: 0.6255\n",
      "Epoch 720/1000 | Loss: 1.3108 | Train Acc: 0.6254 | Val Acc: 0.6257\n",
      "Epoch 721/1000 | Loss: 1.3103 | Train Acc: 0.6254 | Val Acc: 0.6258\n",
      "Epoch 722/1000 | Loss: 1.3099 | Train Acc: 0.6255 | Val Acc: 0.6257\n",
      "Epoch 723/1000 | Loss: 1.3094 | Train Acc: 0.6255 | Val Acc: 0.6262\n",
      "Epoch 724/1000 | Loss: 1.3089 | Train Acc: 0.6255 | Val Acc: 0.6262\n",
      "Epoch 725/1000 | Loss: 1.3085 | Train Acc: 0.6257 | Val Acc: 0.6262\n",
      "Epoch 726/1000 | Loss: 1.3081 | Train Acc: 0.6257 | Val Acc: 0.6263\n",
      "Epoch 727/1000 | Loss: 1.3076 | Train Acc: 0.6257 | Val Acc: 0.6263\n",
      "Epoch 728/1000 | Loss: 1.3072 | Train Acc: 0.6258 | Val Acc: 0.6265\n",
      "Epoch 729/1000 | Loss: 1.3068 | Train Acc: 0.6260 | Val Acc: 0.6265\n",
      "Epoch 730/1000 | Loss: 1.3064 | Train Acc: 0.6260 | Val Acc: 0.6265\n",
      "Epoch 731/1000 | Loss: 1.3059 | Train Acc: 0.6261 | Val Acc: 0.6267\n",
      "Epoch 732/1000 | Loss: 1.3055 | Train Acc: 0.6262 | Val Acc: 0.6268\n",
      "Epoch 733/1000 | Loss: 1.3050 | Train Acc: 0.6263 | Val Acc: 0.6268\n",
      "Epoch 734/1000 | Loss: 1.3046 | Train Acc: 0.6264 | Val Acc: 0.6272\n",
      "Epoch 735/1000 | Loss: 1.3042 | Train Acc: 0.6265 | Val Acc: 0.6272\n",
      "Epoch 736/1000 | Loss: 1.3038 | Train Acc: 0.6266 | Val Acc: 0.6272\n",
      "Epoch 737/1000 | Loss: 1.3033 | Train Acc: 0.6267 | Val Acc: 0.6280\n",
      "Epoch 738/1000 | Loss: 1.3029 | Train Acc: 0.6269 | Val Acc: 0.6280\n",
      "Epoch 739/1000 | Loss: 1.3025 | Train Acc: 0.6270 | Val Acc: 0.6280\n",
      "Epoch 740/1000 | Loss: 1.3020 | Train Acc: 0.6271 | Val Acc: 0.6282\n",
      "Epoch 741/1000 | Loss: 1.3016 | Train Acc: 0.6273 | Val Acc: 0.6283\n",
      "Epoch 742/1000 | Loss: 1.3012 | Train Acc: 0.6273 | Val Acc: 0.6283\n",
      "Epoch 743/1000 | Loss: 1.3007 | Train Acc: 0.6274 | Val Acc: 0.6283\n",
      "Epoch 744/1000 | Loss: 1.3003 | Train Acc: 0.6274 | Val Acc: 0.6282\n",
      "Epoch 745/1000 | Loss: 1.2999 | Train Acc: 0.6275 | Val Acc: 0.6283\n",
      "Epoch 746/1000 | Loss: 1.2995 | Train Acc: 0.6275 | Val Acc: 0.6285\n",
      "Epoch 747/1000 | Loss: 1.2991 | Train Acc: 0.6277 | Val Acc: 0.6287\n",
      "Epoch 748/1000 | Loss: 1.2987 | Train Acc: 0.6277 | Val Acc: 0.6288\n",
      "Epoch 749/1000 | Loss: 1.2982 | Train Acc: 0.6277 | Val Acc: 0.6288\n",
      "Epoch 750/1000 | Loss: 1.2978 | Train Acc: 0.6279 | Val Acc: 0.6288\n",
      "Epoch 751/1000 | Loss: 1.2974 | Train Acc: 0.6279 | Val Acc: 0.6285\n",
      "Epoch 752/1000 | Loss: 1.2970 | Train Acc: 0.6280 | Val Acc: 0.6285\n",
      "Epoch 753/1000 | Loss: 1.2965 | Train Acc: 0.6281 | Val Acc: 0.6285\n",
      "Epoch 754/1000 | Loss: 1.2961 | Train Acc: 0.6282 | Val Acc: 0.6285\n",
      "Epoch 755/1000 | Loss: 1.2957 | Train Acc: 0.6283 | Val Acc: 0.6287\n",
      "Epoch 756/1000 | Loss: 1.2953 | Train Acc: 0.6284 | Val Acc: 0.6288\n",
      "Epoch 757/1000 | Loss: 1.2949 | Train Acc: 0.6286 | Val Acc: 0.6288\n",
      "Epoch 758/1000 | Loss: 1.2945 | Train Acc: 0.6287 | Val Acc: 0.6287\n",
      "Epoch 759/1000 | Loss: 1.2941 | Train Acc: 0.6288 | Val Acc: 0.6288\n",
      "Epoch 760/1000 | Loss: 1.2936 | Train Acc: 0.6289 | Val Acc: 0.6288\n",
      "Epoch 761/1000 | Loss: 1.2932 | Train Acc: 0.6289 | Val Acc: 0.6288\n",
      "Epoch 762/1000 | Loss: 1.2928 | Train Acc: 0.6289 | Val Acc: 0.6290\n",
      "Epoch 763/1000 | Loss: 1.2924 | Train Acc: 0.6290 | Val Acc: 0.6290\n",
      "Epoch 764/1000 | Loss: 1.2920 | Train Acc: 0.6291 | Val Acc: 0.6292\n",
      "Epoch 765/1000 | Loss: 1.2916 | Train Acc: 0.6292 | Val Acc: 0.6292\n",
      "Epoch 766/1000 | Loss: 1.2912 | Train Acc: 0.6293 | Val Acc: 0.6295\n",
      "Epoch 767/1000 | Loss: 1.2908 | Train Acc: 0.6294 | Val Acc: 0.6295\n",
      "Epoch 768/1000 | Loss: 1.2904 | Train Acc: 0.6294 | Val Acc: 0.6297\n",
      "Epoch 769/1000 | Loss: 1.2899 | Train Acc: 0.6295 | Val Acc: 0.6298\n",
      "Epoch 770/1000 | Loss: 1.2895 | Train Acc: 0.6295 | Val Acc: 0.6298\n",
      "Epoch 771/1000 | Loss: 1.2891 | Train Acc: 0.6296 | Val Acc: 0.6302\n",
      "Epoch 772/1000 | Loss: 1.2888 | Train Acc: 0.6297 | Val Acc: 0.6303\n",
      "Epoch 773/1000 | Loss: 1.2883 | Train Acc: 0.6298 | Val Acc: 0.6303\n",
      "Epoch 774/1000 | Loss: 1.2879 | Train Acc: 0.6299 | Val Acc: 0.6303\n",
      "Epoch 775/1000 | Loss: 1.2875 | Train Acc: 0.6300 | Val Acc: 0.6303\n",
      "Epoch 776/1000 | Loss: 1.2871 | Train Acc: 0.6302 | Val Acc: 0.6305\n",
      "Epoch 777/1000 | Loss: 1.2867 | Train Acc: 0.6304 | Val Acc: 0.6305\n",
      "Epoch 778/1000 | Loss: 1.2863 | Train Acc: 0.6305 | Val Acc: 0.6308\n",
      "Epoch 779/1000 | Loss: 1.2859 | Train Acc: 0.6306 | Val Acc: 0.6310\n",
      "Epoch 780/1000 | Loss: 1.2855 | Train Acc: 0.6307 | Val Acc: 0.6310\n",
      "Epoch 781/1000 | Loss: 1.2851 | Train Acc: 0.6307 | Val Acc: 0.6310\n",
      "Epoch 782/1000 | Loss: 1.2847 | Train Acc: 0.6308 | Val Acc: 0.6310\n",
      "Epoch 783/1000 | Loss: 1.2843 | Train Acc: 0.6308 | Val Acc: 0.6308\n",
      "Epoch 784/1000 | Loss: 1.2839 | Train Acc: 0.6309 | Val Acc: 0.6308\n",
      "Epoch 785/1000 | Loss: 1.2835 | Train Acc: 0.6310 | Val Acc: 0.6308\n",
      "Epoch 786/1000 | Loss: 1.2831 | Train Acc: 0.6311 | Val Acc: 0.6308\n",
      "Epoch 787/1000 | Loss: 1.2827 | Train Acc: 0.6314 | Val Acc: 0.6308\n",
      "Epoch 788/1000 | Loss: 1.2823 | Train Acc: 0.6316 | Val Acc: 0.6310\n",
      "Epoch 789/1000 | Loss: 1.2819 | Train Acc: 0.6317 | Val Acc: 0.6312\n",
      "Epoch 790/1000 | Loss: 1.2815 | Train Acc: 0.6318 | Val Acc: 0.6313\n",
      "Epoch 791/1000 | Loss: 1.2811 | Train Acc: 0.6319 | Val Acc: 0.6313\n",
      "Epoch 792/1000 | Loss: 1.2807 | Train Acc: 0.6321 | Val Acc: 0.6315\n",
      "Epoch 793/1000 | Loss: 1.2804 | Train Acc: 0.6321 | Val Acc: 0.6315\n",
      "Epoch 794/1000 | Loss: 1.2800 | Train Acc: 0.6321 | Val Acc: 0.6315\n",
      "Epoch 795/1000 | Loss: 1.2796 | Train Acc: 0.6322 | Val Acc: 0.6315\n",
      "Epoch 796/1000 | Loss: 1.2792 | Train Acc: 0.6323 | Val Acc: 0.6315\n",
      "Epoch 797/1000 | Loss: 1.2788 | Train Acc: 0.6324 | Val Acc: 0.6315\n",
      "Epoch 798/1000 | Loss: 1.2784 | Train Acc: 0.6326 | Val Acc: 0.6315\n",
      "Epoch 799/1000 | Loss: 1.2780 | Train Acc: 0.6326 | Val Acc: 0.6317\n",
      "Epoch 800/1000 | Loss: 1.2777 | Train Acc: 0.6328 | Val Acc: 0.6317\n",
      "Epoch 801/1000 | Loss: 1.2773 | Train Acc: 0.6328 | Val Acc: 0.6317\n",
      "Epoch 802/1000 | Loss: 1.2769 | Train Acc: 0.6328 | Val Acc: 0.6317\n",
      "Epoch 803/1000 | Loss: 1.2765 | Train Acc: 0.6330 | Val Acc: 0.6317\n",
      "Epoch 804/1000 | Loss: 1.2761 | Train Acc: 0.6330 | Val Acc: 0.6317\n",
      "Epoch 805/1000 | Loss: 1.2757 | Train Acc: 0.6331 | Val Acc: 0.6320\n",
      "Epoch 806/1000 | Loss: 1.2753 | Train Acc: 0.6331 | Val Acc: 0.6323\n",
      "Epoch 807/1000 | Loss: 1.2749 | Train Acc: 0.6331 | Val Acc: 0.6322\n",
      "Epoch 808/1000 | Loss: 1.2746 | Train Acc: 0.6332 | Val Acc: 0.6322\n",
      "Epoch 809/1000 | Loss: 1.2742 | Train Acc: 0.6333 | Val Acc: 0.6322\n",
      "Epoch 810/1000 | Loss: 1.2738 | Train Acc: 0.6333 | Val Acc: 0.6322\n",
      "Epoch 811/1000 | Loss: 1.2734 | Train Acc: 0.6334 | Val Acc: 0.6323\n",
      "Epoch 812/1000 | Loss: 1.2730 | Train Acc: 0.6336 | Val Acc: 0.6323\n",
      "Epoch 813/1000 | Loss: 1.2726 | Train Acc: 0.6336 | Val Acc: 0.6323\n",
      "Epoch 814/1000 | Loss: 1.2723 | Train Acc: 0.6337 | Val Acc: 0.6325\n",
      "Epoch 815/1000 | Loss: 1.2719 | Train Acc: 0.6338 | Val Acc: 0.6325\n",
      "Epoch 816/1000 | Loss: 1.2715 | Train Acc: 0.6339 | Val Acc: 0.6328\n",
      "Epoch 817/1000 | Loss: 1.2712 | Train Acc: 0.6339 | Val Acc: 0.6328\n",
      "Epoch 818/1000 | Loss: 1.2707 | Train Acc: 0.6340 | Val Acc: 0.6328\n",
      "Epoch 819/1000 | Loss: 1.2704 | Train Acc: 0.6341 | Val Acc: 0.6328\n",
      "Epoch 820/1000 | Loss: 1.2700 | Train Acc: 0.6341 | Val Acc: 0.6330\n",
      "Epoch 821/1000 | Loss: 1.2696 | Train Acc: 0.6342 | Val Acc: 0.6328\n",
      "Epoch 822/1000 | Loss: 1.2693 | Train Acc: 0.6343 | Val Acc: 0.6328\n",
      "Epoch 823/1000 | Loss: 1.2689 | Train Acc: 0.6344 | Val Acc: 0.6328\n",
      "Epoch 824/1000 | Loss: 1.2685 | Train Acc: 0.6345 | Val Acc: 0.6328\n",
      "Epoch 825/1000 | Loss: 1.2681 | Train Acc: 0.6347 | Val Acc: 0.6330\n",
      "Epoch 826/1000 | Loss: 1.2678 | Train Acc: 0.6347 | Val Acc: 0.6330\n",
      "Epoch 827/1000 | Loss: 1.2674 | Train Acc: 0.6348 | Val Acc: 0.6330\n",
      "Epoch 828/1000 | Loss: 1.2670 | Train Acc: 0.6349 | Val Acc: 0.6330\n",
      "Epoch 829/1000 | Loss: 1.2666 | Train Acc: 0.6349 | Val Acc: 0.6333\n",
      "Epoch 830/1000 | Loss: 1.2663 | Train Acc: 0.6350 | Val Acc: 0.6335\n",
      "Epoch 831/1000 | Loss: 1.2659 | Train Acc: 0.6351 | Val Acc: 0.6337\n",
      "Epoch 832/1000 | Loss: 1.2655 | Train Acc: 0.6351 | Val Acc: 0.6340\n",
      "Epoch 833/1000 | Loss: 1.2652 | Train Acc: 0.6352 | Val Acc: 0.6337\n",
      "Epoch 834/1000 | Loss: 1.2648 | Train Acc: 0.6353 | Val Acc: 0.6337\n",
      "Epoch 835/1000 | Loss: 1.2645 | Train Acc: 0.6354 | Val Acc: 0.6337\n",
      "Epoch 836/1000 | Loss: 1.2641 | Train Acc: 0.6354 | Val Acc: 0.6337\n",
      "Epoch 837/1000 | Loss: 1.2637 | Train Acc: 0.6356 | Val Acc: 0.6337\n",
      "Epoch 838/1000 | Loss: 1.2633 | Train Acc: 0.6355 | Val Acc: 0.6337\n",
      "Epoch 839/1000 | Loss: 1.2630 | Train Acc: 0.6356 | Val Acc: 0.6338\n",
      "Epoch 840/1000 | Loss: 1.2626 | Train Acc: 0.6358 | Val Acc: 0.6338\n",
      "Epoch 841/1000 | Loss: 1.2622 | Train Acc: 0.6358 | Val Acc: 0.6337\n",
      "Epoch 842/1000 | Loss: 1.2619 | Train Acc: 0.6359 | Val Acc: 0.6337\n",
      "Epoch 843/1000 | Loss: 1.2615 | Train Acc: 0.6360 | Val Acc: 0.6337\n",
      "Epoch 844/1000 | Loss: 1.2612 | Train Acc: 0.6360 | Val Acc: 0.6337\n",
      "Epoch 845/1000 | Loss: 1.2608 | Train Acc: 0.6361 | Val Acc: 0.6337\n",
      "Epoch 846/1000 | Loss: 1.2605 | Train Acc: 0.6361 | Val Acc: 0.6338\n",
      "Epoch 847/1000 | Loss: 1.2601 | Train Acc: 0.6361 | Val Acc: 0.6338\n",
      "Epoch 848/1000 | Loss: 1.2597 | Train Acc: 0.6362 | Val Acc: 0.6340\n",
      "Epoch 849/1000 | Loss: 1.2593 | Train Acc: 0.6363 | Val Acc: 0.6340\n",
      "Epoch 850/1000 | Loss: 1.2590 | Train Acc: 0.6365 | Val Acc: 0.6340\n",
      "Epoch 851/1000 | Loss: 1.2586 | Train Acc: 0.6365 | Val Acc: 0.6340\n",
      "Epoch 852/1000 | Loss: 1.2583 | Train Acc: 0.6367 | Val Acc: 0.6340\n",
      "Epoch 853/1000 | Loss: 1.2580 | Train Acc: 0.6368 | Val Acc: 0.6342\n",
      "Epoch 854/1000 | Loss: 1.2576 | Train Acc: 0.6369 | Val Acc: 0.6345\n",
      "Epoch 855/1000 | Loss: 1.2572 | Train Acc: 0.6370 | Val Acc: 0.6345\n",
      "Epoch 856/1000 | Loss: 1.2569 | Train Acc: 0.6371 | Val Acc: 0.6347\n",
      "Epoch 857/1000 | Loss: 1.2565 | Train Acc: 0.6373 | Val Acc: 0.6347\n",
      "Epoch 858/1000 | Loss: 1.2562 | Train Acc: 0.6374 | Val Acc: 0.6350\n",
      "Epoch 859/1000 | Loss: 1.2558 | Train Acc: 0.6375 | Val Acc: 0.6350\n",
      "Epoch 860/1000 | Loss: 1.2554 | Train Acc: 0.6376 | Val Acc: 0.6350\n",
      "Epoch 861/1000 | Loss: 1.2551 | Train Acc: 0.6378 | Val Acc: 0.6355\n",
      "Epoch 862/1000 | Loss: 1.2548 | Train Acc: 0.6379 | Val Acc: 0.6355\n",
      "Epoch 863/1000 | Loss: 1.2544 | Train Acc: 0.6379 | Val Acc: 0.6353\n",
      "Epoch 864/1000 | Loss: 1.2540 | Train Acc: 0.6380 | Val Acc: 0.6353\n",
      "Epoch 865/1000 | Loss: 1.2537 | Train Acc: 0.6381 | Val Acc: 0.6353\n",
      "Epoch 866/1000 | Loss: 1.2533 | Train Acc: 0.6381 | Val Acc: 0.6353\n",
      "Epoch 867/1000 | Loss: 1.2530 | Train Acc: 0.6382 | Val Acc: 0.6353\n",
      "Epoch 868/1000 | Loss: 1.2526 | Train Acc: 0.6382 | Val Acc: 0.6352\n",
      "Epoch 869/1000 | Loss: 1.2523 | Train Acc: 0.6383 | Val Acc: 0.6350\n",
      "Epoch 870/1000 | Loss: 1.2519 | Train Acc: 0.6384 | Val Acc: 0.6350\n",
      "Epoch 871/1000 | Loss: 1.2516 | Train Acc: 0.6384 | Val Acc: 0.6350\n",
      "Epoch 872/1000 | Loss: 1.2512 | Train Acc: 0.6384 | Val Acc: 0.6352\n",
      "Epoch 873/1000 | Loss: 1.2509 | Train Acc: 0.6385 | Val Acc: 0.6353\n",
      "Epoch 874/1000 | Loss: 1.2506 | Train Acc: 0.6385 | Val Acc: 0.6357\n",
      "Epoch 875/1000 | Loss: 1.2502 | Train Acc: 0.6386 | Val Acc: 0.6357\n",
      "Epoch 876/1000 | Loss: 1.2499 | Train Acc: 0.6386 | Val Acc: 0.6358\n",
      "Epoch 877/1000 | Loss: 1.2495 | Train Acc: 0.6387 | Val Acc: 0.6362\n",
      "Epoch 878/1000 | Loss: 1.2492 | Train Acc: 0.6388 | Val Acc: 0.6362\n",
      "Epoch 879/1000 | Loss: 1.2488 | Train Acc: 0.6389 | Val Acc: 0.6367\n",
      "Epoch 880/1000 | Loss: 1.2485 | Train Acc: 0.6390 | Val Acc: 0.6367\n",
      "Epoch 881/1000 | Loss: 1.2482 | Train Acc: 0.6390 | Val Acc: 0.6367\n",
      "Epoch 882/1000 | Loss: 1.2478 | Train Acc: 0.6390 | Val Acc: 0.6367\n",
      "Epoch 883/1000 | Loss: 1.2475 | Train Acc: 0.6391 | Val Acc: 0.6367\n",
      "Epoch 884/1000 | Loss: 1.2471 | Train Acc: 0.6391 | Val Acc: 0.6367\n",
      "Epoch 885/1000 | Loss: 1.2468 | Train Acc: 0.6391 | Val Acc: 0.6368\n",
      "Epoch 886/1000 | Loss: 1.2465 | Train Acc: 0.6391 | Val Acc: 0.6372\n",
      "Epoch 887/1000 | Loss: 1.2461 | Train Acc: 0.6392 | Val Acc: 0.6372\n",
      "Epoch 888/1000 | Loss: 1.2458 | Train Acc: 0.6392 | Val Acc: 0.6372\n",
      "Epoch 889/1000 | Loss: 1.2454 | Train Acc: 0.6393 | Val Acc: 0.6375\n",
      "Epoch 890/1000 | Loss: 1.2451 | Train Acc: 0.6393 | Val Acc: 0.6375\n",
      "Epoch 891/1000 | Loss: 1.2448 | Train Acc: 0.6395 | Val Acc: 0.6377\n",
      "Epoch 892/1000 | Loss: 1.2444 | Train Acc: 0.6396 | Val Acc: 0.6382\n",
      "Epoch 893/1000 | Loss: 1.2441 | Train Acc: 0.6396 | Val Acc: 0.6382\n",
      "Epoch 894/1000 | Loss: 1.2437 | Train Acc: 0.6397 | Val Acc: 0.6383\n",
      "Epoch 895/1000 | Loss: 1.2434 | Train Acc: 0.6399 | Val Acc: 0.6383\n",
      "Epoch 896/1000 | Loss: 1.2431 | Train Acc: 0.6400 | Val Acc: 0.6383\n",
      "Epoch 897/1000 | Loss: 1.2428 | Train Acc: 0.6401 | Val Acc: 0.6383\n",
      "Epoch 898/1000 | Loss: 1.2424 | Train Acc: 0.6402 | Val Acc: 0.6385\n",
      "Epoch 899/1000 | Loss: 1.2421 | Train Acc: 0.6402 | Val Acc: 0.6383\n",
      "Epoch 900/1000 | Loss: 1.2417 | Train Acc: 0.6403 | Val Acc: 0.6382\n",
      "Epoch 901/1000 | Loss: 1.2414 | Train Acc: 0.6404 | Val Acc: 0.6382\n",
      "Epoch 902/1000 | Loss: 1.2411 | Train Acc: 0.6404 | Val Acc: 0.6382\n",
      "Epoch 903/1000 | Loss: 1.2407 | Train Acc: 0.6405 | Val Acc: 0.6382\n",
      "Epoch 904/1000 | Loss: 1.2404 | Train Acc: 0.6405 | Val Acc: 0.6382\n",
      "Epoch 905/1000 | Loss: 1.2401 | Train Acc: 0.6407 | Val Acc: 0.6382\n",
      "Epoch 906/1000 | Loss: 1.2398 | Train Acc: 0.6408 | Val Acc: 0.6385\n",
      "Epoch 907/1000 | Loss: 1.2394 | Train Acc: 0.6409 | Val Acc: 0.6387\n",
      "Epoch 908/1000 | Loss: 1.2391 | Train Acc: 0.6409 | Val Acc: 0.6385\n",
      "Epoch 909/1000 | Loss: 1.2388 | Train Acc: 0.6410 | Val Acc: 0.6387\n",
      "Epoch 910/1000 | Loss: 1.2384 | Train Acc: 0.6411 | Val Acc: 0.6388\n",
      "Epoch 911/1000 | Loss: 1.2381 | Train Acc: 0.6413 | Val Acc: 0.6388\n",
      "Epoch 912/1000 | Loss: 1.2378 | Train Acc: 0.6413 | Val Acc: 0.6388\n",
      "Epoch 913/1000 | Loss: 1.2374 | Train Acc: 0.6414 | Val Acc: 0.6392\n",
      "Epoch 914/1000 | Loss: 1.2371 | Train Acc: 0.6414 | Val Acc: 0.6392\n",
      "Epoch 915/1000 | Loss: 1.2368 | Train Acc: 0.6414 | Val Acc: 0.6392\n",
      "Epoch 916/1000 | Loss: 1.2365 | Train Acc: 0.6415 | Val Acc: 0.6397\n",
      "Epoch 917/1000 | Loss: 1.2361 | Train Acc: 0.6416 | Val Acc: 0.6397\n",
      "Epoch 918/1000 | Loss: 1.2358 | Train Acc: 0.6417 | Val Acc: 0.6397\n",
      "Epoch 919/1000 | Loss: 1.2355 | Train Acc: 0.6418 | Val Acc: 0.6397\n",
      "Epoch 920/1000 | Loss: 1.2352 | Train Acc: 0.6419 | Val Acc: 0.6400\n",
      "Epoch 921/1000 | Loss: 1.2349 | Train Acc: 0.6420 | Val Acc: 0.6400\n",
      "Epoch 922/1000 | Loss: 1.2345 | Train Acc: 0.6420 | Val Acc: 0.6400\n",
      "Epoch 923/1000 | Loss: 1.2342 | Train Acc: 0.6421 | Val Acc: 0.6400\n",
      "Epoch 924/1000 | Loss: 1.2339 | Train Acc: 0.6422 | Val Acc: 0.6400\n",
      "Epoch 925/1000 | Loss: 1.2335 | Train Acc: 0.6424 | Val Acc: 0.6400\n",
      "Epoch 926/1000 | Loss: 1.2332 | Train Acc: 0.6424 | Val Acc: 0.6400\n",
      "Epoch 927/1000 | Loss: 1.2329 | Train Acc: 0.6425 | Val Acc: 0.6400\n",
      "Epoch 928/1000 | Loss: 1.2326 | Train Acc: 0.6426 | Val Acc: 0.6400\n",
      "Epoch 929/1000 | Loss: 1.2323 | Train Acc: 0.6427 | Val Acc: 0.6398\n",
      "Epoch 930/1000 | Loss: 1.2319 | Train Acc: 0.6428 | Val Acc: 0.6398\n",
      "Epoch 931/1000 | Loss: 1.2316 | Train Acc: 0.6428 | Val Acc: 0.6400\n",
      "Epoch 932/1000 | Loss: 1.2313 | Train Acc: 0.6428 | Val Acc: 0.6400\n",
      "Epoch 933/1000 | Loss: 1.2310 | Train Acc: 0.6429 | Val Acc: 0.6400\n",
      "Epoch 934/1000 | Loss: 1.2307 | Train Acc: 0.6430 | Val Acc: 0.6402\n",
      "Epoch 935/1000 | Loss: 1.2304 | Train Acc: 0.6430 | Val Acc: 0.6402\n",
      "Epoch 936/1000 | Loss: 1.2301 | Train Acc: 0.6431 | Val Acc: 0.6403\n",
      "Epoch 937/1000 | Loss: 1.2298 | Train Acc: 0.6431 | Val Acc: 0.6407\n",
      "Epoch 938/1000 | Loss: 1.2294 | Train Acc: 0.6432 | Val Acc: 0.6410\n",
      "Epoch 939/1000 | Loss: 1.2291 | Train Acc: 0.6433 | Val Acc: 0.6410\n",
      "Epoch 940/1000 | Loss: 1.2288 | Train Acc: 0.6434 | Val Acc: 0.6410\n",
      "Epoch 941/1000 | Loss: 1.2285 | Train Acc: 0.6435 | Val Acc: 0.6410\n",
      "Epoch 942/1000 | Loss: 1.2282 | Train Acc: 0.6436 | Val Acc: 0.6410\n",
      "Epoch 943/1000 | Loss: 1.2279 | Train Acc: 0.6436 | Val Acc: 0.6410\n",
      "Epoch 944/1000 | Loss: 1.2276 | Train Acc: 0.6436 | Val Acc: 0.6412\n",
      "Epoch 945/1000 | Loss: 1.2272 | Train Acc: 0.6437 | Val Acc: 0.6413\n",
      "Epoch 946/1000 | Loss: 1.2269 | Train Acc: 0.6438 | Val Acc: 0.6413\n",
      "Epoch 947/1000 | Loss: 1.2266 | Train Acc: 0.6439 | Val Acc: 0.6413\n",
      "Epoch 948/1000 | Loss: 1.2263 | Train Acc: 0.6439 | Val Acc: 0.6413\n",
      "Epoch 949/1000 | Loss: 1.2260 | Train Acc: 0.6440 | Val Acc: 0.6413\n",
      "Epoch 950/1000 | Loss: 1.2257 | Train Acc: 0.6440 | Val Acc: 0.6413\n",
      "Epoch 951/1000 | Loss: 1.2254 | Train Acc: 0.6441 | Val Acc: 0.6415\n",
      "Epoch 952/1000 | Loss: 1.2251 | Train Acc: 0.6442 | Val Acc: 0.6417\n",
      "Epoch 953/1000 | Loss: 1.2248 | Train Acc: 0.6442 | Val Acc: 0.6417\n",
      "Epoch 954/1000 | Loss: 1.2244 | Train Acc: 0.6443 | Val Acc: 0.6418\n",
      "Epoch 955/1000 | Loss: 1.2241 | Train Acc: 0.6443 | Val Acc: 0.6418\n",
      "Epoch 956/1000 | Loss: 1.2238 | Train Acc: 0.6444 | Val Acc: 0.6420\n",
      "Epoch 957/1000 | Loss: 1.2235 | Train Acc: 0.6444 | Val Acc: 0.6420\n",
      "Epoch 958/1000 | Loss: 1.2232 | Train Acc: 0.6444 | Val Acc: 0.6423\n",
      "Epoch 959/1000 | Loss: 1.2229 | Train Acc: 0.6444 | Val Acc: 0.6423\n",
      "Epoch 960/1000 | Loss: 1.2226 | Train Acc: 0.6445 | Val Acc: 0.6422\n",
      "Epoch 961/1000 | Loss: 1.2223 | Train Acc: 0.6446 | Val Acc: 0.6422\n",
      "Epoch 962/1000 | Loss: 1.2220 | Train Acc: 0.6446 | Val Acc: 0.6422\n",
      "Epoch 963/1000 | Loss: 1.2217 | Train Acc: 0.6446 | Val Acc: 0.6422\n",
      "Epoch 964/1000 | Loss: 1.2214 | Train Acc: 0.6447 | Val Acc: 0.6422\n",
      "Epoch 965/1000 | Loss: 1.2211 | Train Acc: 0.6448 | Val Acc: 0.6422\n",
      "Epoch 966/1000 | Loss: 1.2208 | Train Acc: 0.6449 | Val Acc: 0.6423\n",
      "Epoch 967/1000 | Loss: 1.2205 | Train Acc: 0.6449 | Val Acc: 0.6423\n",
      "Epoch 968/1000 | Loss: 1.2202 | Train Acc: 0.6450 | Val Acc: 0.6423\n",
      "Epoch 969/1000 | Loss: 1.2199 | Train Acc: 0.6452 | Val Acc: 0.6425\n",
      "Epoch 970/1000 | Loss: 1.2196 | Train Acc: 0.6453 | Val Acc: 0.6425\n",
      "Epoch 971/1000 | Loss: 1.2192 | Train Acc: 0.6454 | Val Acc: 0.6428\n",
      "Epoch 972/1000 | Loss: 1.2190 | Train Acc: 0.6455 | Val Acc: 0.6430\n",
      "Epoch 973/1000 | Loss: 1.2187 | Train Acc: 0.6455 | Val Acc: 0.6432\n",
      "Epoch 974/1000 | Loss: 1.2184 | Train Acc: 0.6455 | Val Acc: 0.6433\n",
      "Epoch 975/1000 | Loss: 1.2180 | Train Acc: 0.6456 | Val Acc: 0.6435\n",
      "Epoch 976/1000 | Loss: 1.2178 | Train Acc: 0.6456 | Val Acc: 0.6433\n",
      "Epoch 977/1000 | Loss: 1.2175 | Train Acc: 0.6456 | Val Acc: 0.6433\n",
      "Epoch 978/1000 | Loss: 1.2172 | Train Acc: 0.6457 | Val Acc: 0.6433\n",
      "Epoch 979/1000 | Loss: 1.2169 | Train Acc: 0.6458 | Val Acc: 0.6432\n",
      "Epoch 980/1000 | Loss: 1.2166 | Train Acc: 0.6459 | Val Acc: 0.6432\n",
      "Epoch 981/1000 | Loss: 1.2163 | Train Acc: 0.6459 | Val Acc: 0.6432\n",
      "Epoch 982/1000 | Loss: 1.2160 | Train Acc: 0.6459 | Val Acc: 0.6432\n",
      "Epoch 983/1000 | Loss: 1.2157 | Train Acc: 0.6460 | Val Acc: 0.6433\n",
      "Epoch 984/1000 | Loss: 1.2154 | Train Acc: 0.6460 | Val Acc: 0.6433\n",
      "Epoch 985/1000 | Loss: 1.2151 | Train Acc: 0.6461 | Val Acc: 0.6437\n",
      "Epoch 986/1000 | Loss: 1.2148 | Train Acc: 0.6462 | Val Acc: 0.6437\n",
      "Epoch 987/1000 | Loss: 1.2145 | Train Acc: 0.6463 | Val Acc: 0.6438\n",
      "Epoch 988/1000 | Loss: 1.2142 | Train Acc: 0.6464 | Val Acc: 0.6442\n",
      "Epoch 989/1000 | Loss: 1.2139 | Train Acc: 0.6465 | Val Acc: 0.6440\n",
      "Epoch 990/1000 | Loss: 1.2136 | Train Acc: 0.6465 | Val Acc: 0.6442\n",
      "Epoch 991/1000 | Loss: 1.2133 | Train Acc: 0.6465 | Val Acc: 0.6442\n",
      "Epoch 992/1000 | Loss: 1.2130 | Train Acc: 0.6466 | Val Acc: 0.6442\n",
      "Epoch 993/1000 | Loss: 1.2127 | Train Acc: 0.6466 | Val Acc: 0.6442\n",
      "Epoch 994/1000 | Loss: 1.2124 | Train Acc: 0.6466 | Val Acc: 0.6443\n",
      "Epoch 995/1000 | Loss: 1.2122 | Train Acc: 0.6466 | Val Acc: 0.6445\n",
      "Epoch 996/1000 | Loss: 1.2119 | Train Acc: 0.6467 | Val Acc: 0.6445\n",
      "Epoch 997/1000 | Loss: 1.2116 | Train Acc: 0.6467 | Val Acc: 0.6445\n",
      "Epoch 998/1000 | Loss: 1.2113 | Train Acc: 0.6468 | Val Acc: 0.6447\n",
      "Epoch 999/1000 | Loss: 1.2110 | Train Acc: 0.6469 | Val Acc: 0.6448\n",
      "Epoch 1000/1000 | Loss: 1.2107 | Train Acc: 0.6470 | Val Acc: 0.6450\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(layers=[784, 128, 64, 10])\n",
    "mlp.fit(X_train, y_train, X_val, y_val, epochs=1000, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "128382a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on test data...\n",
      "Final Test Accuracy (with sklearn): 0.6451\n"
     ]
    }
   ],
   "source": [
    "# --- Test the trained model using scikit-learn's metric ---\n",
    "\n",
    "print(\"\\nEvaluating model on test data...\")\n",
    "\n",
    "# Import the necessary function\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test = mlp.forward(X_test / 255.0)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred_test_labels = np.argmax(y_pred_test, axis=1)\n",
    "\n",
    "# Calculate final test accuracy using sklearn\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test_labels)\n",
    "\n",
    "print(f\"Final Test Accuracy (with sklearn): {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
